Abstract:Estimating spot covariance is an important issue to study, especially with theincreasing availability of high-frequency  nancial data. We study the estimationof spot covariance using a kernel method for high-frequency data. In particular,we consider  rst the kernel weighted version of realized covariance estimatorfor the price process governed by a continuous multivariate semimartingale.Next, we extend it to the threshold kernel estimator of the spot covarianceswhen the underlying price process is a discontinuous multivariate semimartingalewith  nite activity jumps. We derive the asymptotic distribution of theestimators for both  xed and shrinking bandwidth. The estimator in a settingwith jumps has the same rate of convergence as the estimator for di usionprocesses without jumps. A simulation study examines the  nite sample propertiesof the estimators. In addition, we study an application of the estimator in thecontext of covariance forecasting. We discover that the forecasting model withour estimator outperforms a benchmark model in the literature.Keywords:high-frequency data; kernel estimation; jump; forecasting covariance matrix
SEP
Abstract:This paper proposes a dynamic spatial autoregressive quantile model. Usingpredetermined network information, we study dynamic tail event driven risk usinga system of conditional quantile equations. Extending Zhu, Wang, Wang and Härdle(2019), we allow the contemporaneous dependency of nodal responses byincorporating a spatial lag in our model. For example, this is to allow a firm’stail behavior to be connected with a weighted aggregation of the simultaneousreturns of the other firms. In addition, we control for the common factoreffects. The instrumental variable quantile regressive method is used for ourmodel estimation, and the associated asymptotic theory for estimation is alsoprovided. Simulation results show that our model performs well at variousquantile levels with different network structures, especially when the node sizeincreases. Finally, we illustrate our method with an empirical study. We uncoversignificant network effects in the spatial lag among financial institutions.Keywords:Network, Quantile autoregression, Instrumental variables, Dynamic models
SEP
Abstract:Inflation expectation (IE) is often considered to be an important determinant ofactual inflation in modern economic theory, we are interested in investigatingthe main risk factors that determine its dynamics. We fiirst apply a jointarbitrage-free term structure model across different European countries toobtain estimate for country-specific IE. Then we use the two-component andthree-component models to capture the main risk factors. We discover that theextracted common trend for IE is an important driver for each country ofinterest. Moreover a spatial-temporal copula model is  tted to account for thenon-Gaussian dependency across countries. This paper aims to extract informativeestimates for IE and provide good implications for monetary policies.Keywords:in ation expectation; joint yield-curve modeling; factor model; common trend;spatial-temporal copulas
SEP
Abstract:A factor augmented dynamic model for analysing tail behaviour of highdimensional time series is proposed. As a first step, the tail event drivenlatent factors are extracted. In the second step, a VAR (Vectorautoregressionmodel) is carried out to analyse the interaction between these factors and themacroeconomic variables. Furthermore, this methodology also provides thepossibility for central banks to examine the sensitivity between macroeconomicvariables and financial shocks via impulse response analysis. Then thepredictability of our estimator is illustrated. Finally, forecast error variancedecomposition is carried out to investigate the network effect of thesevariables. The interesting findings are: firstly, GDP and Unemployment rate arevery much sensitive to the shock of financial tail event driven factors, whilethese factors are more affected by inflation and short term interest rate.Secondly, financial tail event driven factors play important roles in thenetwork constructed by the extracted factors and the macroeconomic variables.Thirdly, there is more connectedness during financial crisis than in the stableperiods. Compared with median case, the network is more dense in lower quantilelevel.Keywords:Quantile Regression, Expectile Regression, Dynamic Factor Model, Dynamic Network
SEP
Abstract:Modelling dynamic conditional heteroscedasticity is the daily routine in timeseries econometrics. We propose a weighted conditional moment estimation topotentially improve the eciency of the QMLE (quasi maximum likelihoodestimation). The weights of conditional moments are selected based on theanalytical form of optimal instruments, and we nominally decide the optimalinstrument based on the third and fourth moments of the underlying error term.This approach is motivated by the idea of general estimation equations (GEE). Wealso provide an analysis of the eciency of QMLE for the location and varianceparameters. Simulations and applications are conducted to show the betterperformance of our estimators.Keywords:-
SEP
Abstract:We propose a bivariate component GARCH-MIDAS model to estimate the long- andshort-run components of the variances and covariances. The advantage of ourmodel to the existing DCC-based models is that it uses the same form for boththe variances and covariances and that it estimates these momentssimultaneously. We apply this model to obtain long- and short-run factor betasfor industry test portfolios, where the risk factors are the market, SMB, andHML portfolios. We use these betas in cross-sectional analysis of the riskpremia. Among other things, we find that the risk premium related to the short-run market beta is significantly positive, irrespective of the choice of testportfolio. Further, the risk premia for the short-run betas of all the riskfactors are significant outside recessions.Keywords:long-run betas; short-run betas; risk premia; business cycles; component GARCHmodel; MIDAS
SEP
Abstract:For multiple change-points detection of high-dimensional time series, we provideasymptotic theory concerning the consistency and the asymptotic distribution ofthe breakpoint statistics and estimated break sizes. The theory backs up asimple two- step procedure for detecting and estimating multiple change-points.The proposed two-step procedure involves the maximum of a MOSUM (moving sum)type statistics in the  rst step and a CUSUM (cumulative sum) re nement step onan aggregated time series in the second step. Thus, for a  xed time-point, wecan capture both the biggest break across di erent coordinates and aggregatingsimultaneous breaks over multiple coordinates. Extending the existing high-dimensional Gaussian approxima- tion theorem to dependent data with jumps, thetheory allows us to characterize the size and power of our multiple change-pointtest asymptotically. Moreover, we can make inferences on the breakpointsestimates when the break sizes are small. Our theoretical setup incorporatesboth weak temporal and strong or weak cross-sectional dependence and is suitablefor heavy-tailed innovations. A robust long-run covariance matrix estimation isproposed, which can be of independent interest. An application on detectingstructural changes of the U.S. unemployment rate is considered to illus- tratethe usefulness of our method.Keywords:multiple change points detection; temporal and cross-sectional dependence;Gaussian approximation; inference of break locations
SEP
Abstract:We develop a uniform test for detecting and dating explosive behavior of astrictly stationary GARCH(r, s) (generalized autoregressive conditionalheteroskedasticity) process. Namely, we test the null hypothesis of a globallystable GARCH process with constant parameters against an alternative where thereis an ’abnormal’ period with changed parameter values. During this period, thechange may lead to an explosive behavior of the volatility process. It isassumed that both the magnitude and the timing of the breaks are unknown. Wedevelop a double supreme test for the existence of a break, and then provide analgorithm to identify the period of change. Our theoretical results hold undermild moment assumptions on the innovations of the GARCH process. Technically,the existing properties for the QMLE in the GARCH model need to bereinvestigated to hold uniformly over all possible periods of change. The keyresults involve a uniform weak Bahadur representation for the estimatedparameters, which leads to weak convergence of the test statistic to the supremeof a Gaussian Process. In simulations we show that the test has good size andpower for reasonably large time series lengths. We apply the test to Apple assetreturns and Bitcoin returns.Keywords:GARCH, IGARCH, Change-point Analysis, Concentration Inequalities, Uniform Test
SEP
Abstract:In this paper, we study estimation of nonlinear models with cross sectional datausing two-step generalized estimating equations (GEE) in the quasi-maximumlikelihood estimation (QMLE) framework. In the interest of improving efficiency,we propose a grouping estimator to account for the potential spatial correlationin the underlying innovations. We use a Poisson model and a Negative Binomial IImodel for count data and a Probit model for binary response data to demonstratethe GEE procedure. Under mild weak dependency assumptions, results on estimationconsistency and asymptotic normality are provided. Monte Carlo simulations showefficiency gain of our approach in comparison of different estimation methodsfor count data and binary response data. Finally we apply the GEE approach tostudy the determinants of the inflow foreign direct investment (FDI) to China.Keywords:quasi-maximum likelihood estimation; generalized estimating equations; nonlinearmodels; spatial dependence; count data; binary response data; FDI equation
SEP
Abstract:Penalized spline smoothing of time series and its asymptotic properties arestudied. A data-driven algorithm for selecting the smoothing parameter isdeveloped. The proposal is applied to de ne a semiparametric extension of thewell-known Spline- GARCH, called a P-Spline-GARCH, based on the log-datatransformation of the squared returns. It is shown that now the errors processis exponentially strong mixing with  nite moments of all orders. Asymptoticnormality of the P-spline smoother in this context is proved. Practicalrelevance of the proposal is illustrated by data examples and simulation. Theproposal is further applied to value at risk and expected shortfall.Keywords:P-spline smoother, smoothing parameter selection, P-Spline-GARCH, strong mixing,value at risk, expected shortfall
SEP
Abstract:Tail risk protection is in the focus of the financial industry and requiressolid mathematical and statistical tools, especially when a trading strategy isderived. Recent hype driven by machine learning (ML) mechanisms has raised thenecessity to display and understand the functionality of ML tools. In thispaper, we present a dynamic tail risk protection strategy that targets a maximumpredefined level of risk measured by Value-At-Risk while controlling forparticipation in bull market regimes. We propose different weak classifiers,parametric and non-parametric, that estimate the exceedance probability of therisk level from which we derive trading signals in order to hedge tail events.We then compare the different approaches both with statistical and tradingstrategy performance, finally we propose an ensemble classifier that produces ameta tail risk protection strategy improving both generalization and tradingperformance.Keywords:-
SEP
Abstract:We investigate the finite sample performance of sample splitting, cross-fittingand averaging for the estimation of the conditional average treatment effect.Recently proposed methods, so-called meta- learners, make use of machinelearning to estimate different nuisance functions and hence allow for fewerrestrictions on the underlying structure of the data. To limit a potentialoverfitting bias that may result when using machine learning methods, cross-fitting estimators have been proposed. This includes the splitting of the datain different folds to reduce bias and averaging over folds to restoreefficiency. To the best of our knowledge, it is not yet clear how exactly thedata should be split and averaged. We employ a Monte Carlo study with differentdata generation processes and consider twelve different estimators that vary insample-splitting, cross-fitting and averaging procedures. We investigate theperformance of each estimator independently on four different meta-learners: thedoubly-robust-learner, R-learner, T-learner and X-learner. We find that theperformance of all meta-learners heavily depends on the procedure of splittingand averaging. The best performance in terms of mean squared error (MSE) amongthe sample split estimators can be achieved when applying cross-fitting plustaking the median over multiple different sample-splitting iterations. Somemeta-learners exhibit a high variance when the lasso is included in the MLmethods. Excluding the lasso decreases the variance and leads to robust and atleast competitive results.Keywords:causal inference, sample splitting, cross-fitting, sample averaging, machinelearning, simulation study
SEP
Abstract:Cryptocurrencies’ values often respond aggressively to major policy changes, butnone of the existing indices informs on the market risks associated withregulatory changes. In this paper, we quantify the risks originating from newregulations on FinTech and cryptocurrencies (CCs), and analyse their impact onmarket dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX) isconstructed based on policy-related news coverage frequency. The unlabeled newsdata are collected from the top online CC news platforms and further classifiedusing a Latent Dirichlet Allocation model and Hellinger distance. Our resultsshow that the machine-learning-based CRRIX successfully captures major policy-changing moments. The movements for both the VCRIX, a market volatility index,and the CRRIX are synchronous, meaning that the CRRIX could be helpful for allparticipants in the cryptocurrency market. The algorithms and Python code areavailable for research purposes on www.quantlet.de.Keywords:Cryptocurrency, Regulatory Risk, Index, LDA, News Classification
SEP
Abstract:This paper aims to model the joint dynamics of cryptocurrencies in anonstationary setting. In particular, we analyze the role of cointegrationrelationships within a large system of cryptocurrencies in a vector errorcorrection model (VECM) framework. To enable analysis in a dynamic setting, wepropose the COINtensity VECM, a nonlinear VECM specification accounting for avarying systemwide cointegration exposure. Our results show thatcryptocurrencies are indeed cointegrated with a cointegration rank of four. Wealso find that all currencies are affected by these long term equilibriumrelations. A simple statistical arbitrage trading strategy is proposed showing agreat in-sample performance.Keywords:Cointegration, VECM, Nonstationarity, Cryptocurrencies
SEP
Abstract:Many countries have taken non-pharmaceutical interventions (NPIs) to contain thespread of the coronavirus (COVID-19) and push the recovery of nationaleconomies. This paper investigates the effect of these control measures bycomparing five selected countries, China, Italy, Germany, the United Kingdom,and the United States. There is evidence that the degree of early interventionand efficacy of control measures are essential to contain the pandemic. Chinastands out because its early and strictly enforced interventions are effectiveto contain the virus spread. Furthermore, we quantify the causal effect ofdifferent control measures on COVID-19 transmission and work resumption inChina. Surprisingly, digital contact tracing and delegating clear responsibilityto the local community appear to be the two most effective policy measures fordisease containment and work resumption. Public information campaigns and socialdistancing also help to flatten the peak significantly. Moreover, materiallogistics that prevent medical supply shortages provide an additionalconditioning factor for disease containment and work resumption. Fiscal policy,however, is less effective at the early to middle stage of the pandemic.Keywords:COVID-19, coronavirus
SEP
Abstract:Among nonparametric smoothers, there is a well-known correspondence betweenkernel and Fourier series methods, pivoted by the Fourier transform of thekernel. This suggests a similar relationship between kernel and splineestimators. A known special case is the result of Silverman (1984) on theeffective kernel for the classical Reinsch-Schoenberg smoothing spline in thenonparametric regression model. We present an extension by showing that a largeclass of kernel estimators have a spline equivalent, in the sense of identicalasymptotic local behaviour of the weighting coefficients. This general class ofspline smoothers includes also the minimax linear estimator over Sobolevellipsoids. The analysis is carried out for piecewise linear splines andequidistant design.Keywords:Kernel estimator, spline smoothing, filtering coefficients, differentialoperator, Green's function approximation, asymptotic minimax spline
SEP
Abstract:The cryptocurrency market is unique on many levels: Very volatile, frequentlychanging market structure, emerging and vanishing of cryptocurrencies on a dailylevel. Following its development became a difficult task with the success ofcryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the IMFoffers the index SDR and, prior to the EUR, the ECU existed, which was an indexrepresenting the development of European currencies. Index providers decide on afixed number of index constituents which will represent the market segment. Itis a challenge to fix a number and develop rules for the constituents in view ofthe market changes. In the frequently changing CC market, this challenge is evenmore severe. A method relying on the AIC is proposed to quickly react to marketchanges and therefore enable us to create an index, referred to as CRIX, for thecryptocurrency market. CRIX is chosen by model selection such that it representsthe market well to enable each interested party studying economic questions inthis market and to invest into the market. The diversified nature of the CCmarket makes the inclusion of altcoins in the index product critical to improvetracking performance. We have shown that assigning optimal weights to altcoinshelps to reduce the tracking errors of a CC portfolio, despite the fact thattheir market cap is much smaller relative to Bitcoin. The codes used here areavailable via www.quantlet.de.Keywords:Index construction, Model selection, Bitcoin, Cryptocurrency, CRIX, Altcoin
SEP
Abstract:In this paper, we conduct simultaneous inference of the non-parametric part of apartially linear model when the non-parametric component is a multivariateunknown function. Based on semi-parametric estimates of the model, we constructa simultaneous confidence region of the multivariate function for simultaneousinference. The developed methodology is applied to perform simultaneousinference for the U.S. gasoline demand where the income and price variables arecontaminated by Berkson errors. The empirical results strongly suggest that thelinearity of the U.S. gasoline demand is rejected. The results are also used topropose an alternative form for the demand.Keywords:Simultaneous inference, Multivariate function, Simultaneous confidence region,Berkson error, Regression calibration
SEP
Abstract:Financial statement fraud is an area of significant consternation for potentialinvestors, auditing companies, and state regulators. Intelligent systemsfacilitate detecting financial statement fraud and assist the decision-making ofrelevant stakeholders. Previous research detected instances in which financialstatements have been fraudulently misrepresented in managerial comments. Thepaper aims to investigate whether it is possible to develop an enhanced systemfor detecting financial fraud through the combination of information sourcedfrom financial ratios and managerial comments within corporate annual reports.We employ a hierarchical attention network (HAN) with a long short-term memory(LSTM) encoder to extract text features from the Management Discussion andAnalysis (MD&A) section of annual reports. The model is designed to offer twodistinct features. First, it reflects the structured hierarchy of documents,which previous models were unable to capture. Second, the model embodies twodifferent attention mechanisms at the word and sentence level, which allowscontent to be differentiated in terms of its importance in the process ofconstructing the document representation. As a result of its architecture, themodel captures both content and context of managerial comments, which serve assupplementary predictors to financial ratios in the detection of fraudulentreporting. Additionally, the model provides interpretable indicators denoted as“red-flag” sentences, which assist stakeholders in their process of determiningwhether further investigation of a specific annual report is required. Empiricalresults demonstrate that textual features of MD&A sections extracted by HANyield promising classification results and substantially reinforce financialratios.Keywords:fraud detection, financial statements, deep learning, text analytics
SEP
Abstract:Deep learning has substantially advanced the state of the art in computervision, natural language processing, and other fields. The paper examines thepotential of deep learning for exchange rate forecasting. We systematicallycompare long short- term memory networks and gated recurrent units totraditional recurrent network architectures as well as feedforward networks interms of their directional forecasting accuracy and the profitability of tradingmodel predictions. Empirical results indicate the suitability of deep networksfor exchange rate forecasting in general but also evidence the difficulty ofimplementing and tuning corresponding architectures. Especially with regard totrading profit, a simpler neural network may perform as well as if not betterthan a more complex deep neural network.Keywords:Deep learning, Financial time series forecasting, Recurrent neural networks,Foreign exchange rates
SEP
Abstract:This study provides a formal analysis of the customer targeting decision problemin settings where the cost for marketing action is stochastic and proposes aframework to efficiently estimate the decision variables for campaign profitoptimization. Targeting a customer is profitable if the positive impact of themarketing treatment on the customer and the associated profit to the company ishigher than the cost of the treatment. While there is a growing literature ondeveloping causal or uplift models to identify the customers who are impactedmost strongly by the marketing action, no research has investigated optimaltargeting when the costs of the action are uncertain at the time of thetargeting decision. Because marketing incentives are routinely conditioned on apositive response by the customer, e.g. a purchase or contract renewal,stochastic costs are ubiquitous in direct marketing and customer retentioncampaigns. This study makes two contributions to the literature, which areevaluated on a coupon targeting campaign in an e-commerce setting. First, theauthors formally analyze the targeting decision problem under response-dependentcosts. Profit-optimal targeting requires an estimate of the treatment effect onthe customer and an estimate of the customer response probability undertreatment. The empirical results demonstrate that the consideration of treatmentcost substantially increases campaign profit when used for customer targeting incombination with the estimation of the average or customer- level treatmenteffect. Second, the authors propose a framework to jointly estimate thetreatment effect and the response probability combining methods for causalinference with a hurdle mixture model. The proposed causal hurdle model achievescompetitive campaign profit while streamlining model building. The code for theempirical analysis is available on Github.Keywords:Heterogeneous Treatment Effect, Uplift Modeling, Coupon Targeting,Churn/Retention, Campaign Profit
SEP
Abstract:A multivariate quantile regression model with a factor structure is proposed tostudy data with many responses of interest. The factor structure is allowed tovary with the quantile levels, which makes our framework more flexible than theclassical factor models. The model is estimated with the nuclear normregularization in order to accommodate the high dimensionality of data, but theincurred optimization problem can only be efficiently solved in an approximatemanner by off-the-shelf optimization methods. Such a scenario is often seen whenthe empirical risk is non-smooth or the numerical procedure involves expensivesubroutines such as singular value decompo- sition. To ensure that theapproximate estimator accurately estimates the model, non-asymptotic bounds onerror of the the approximate estimator is established. For implementation, anumerical procedure that provably marginalizes the approximate error isproposed. The merits of our model and the proposed numerical procedures aredemonstrated through Monte Carlo experiments and an application to financeinvolving a large pool of asset returns.Keywords:Factor model, quantile regression, non-asymptotic analysis, multivariateregression, nuclear norm regularization
SEP
Abstract:Recently, a number of structured funds have emerged as public-privatepartnerships with the intent of promoting investment in renewable energy inemerging markets. These funds seek to attract institutional investors bytranching the asset pool and issuing senior notes with a high credit quality.Financing of renewable energy (RE) projects is achieved via two channels: smallRE projects are financed indirectly through local banks that draw loans from thefund’s assets, whereas large RE projects are directly financed from the fund. Ina bottom-up Gaussian copula framework, we examine the diversification propertiesand RE exposure of the senior tranche. To this end, we introduce the LH++ model,which combines a homogeneous infinitely granular loan portfolio with a finitenumber of large loans. Using expected tranche percentage notional (which takes asimilar role as the default probability of a loan), tranche prices and tranchesensitivities in RE loans, we analyse the risk profile of the senior tranche. Weshow how the mix of indirect and direct RE investments in the asset pool affectsthe sensitivity of the senior tranche to RE investments and how to balance adesired sensitivity with a target credit quality and target tranche size.Keywords:Renewable energy financing, structured finance, CDO pricing, LH++ model
SEP
Abstract:With growing economic globalization, the modern service sector is in great needof business intelligence for data analytics and computational statistics. Thejoint application of big data analytics, computational statistics and businessintelligence has great potential to make the engineering of advanced servicesystems more efficient. The purpose of this COST issue is to publish high-quality research papers (including reviews) that address the challenges ofservice data analytics with business intelligence in the face of uncertainty andrisk. High quality contributions that are not yet published or that are notunder review by other journals or peer-reviewed conferences have been collected.The resulting topic oriented special issue includes research on businessintelligence and computational statistics, data-driven financial engineering,service data analytics and algorithms for optimizing the business engineering.It also covers implementation issues of managing the service process,computational statistics for risk analysis and novel theoretical andcomputational models, data mining algorithms for risk management relatedbusiness applications.Keywords:Data Analytics, Business Intelligence Systems
SEP
Abstract:The paper estimates banks’ total factor efficiency (TFE) as well as TFE of eachproduction factor by incorporating banks’ overall risk endogenously into bank’sproduction process as undesirable by-product in a Global-SMB Model. Our resultsshow that, compared with a model incorporated with banks’ overall risk, a modelconsidering only on-balance-sheet risk may over-estimate the integrated TFE(TFIE) and under-estimate TFE volatility. Significant heterogeneities of bankTFIE and TFE of each production factor exist among banks of different types andregions, as a result of still prominent unbalanced development of Chinesecommercial banks. Based on the estimated TFIE, the paper further investigatesthe determinants of bank efficiency, and finds that shadow banking, bank size,NPL ratio, loan to deposit ratio, fiscal surplus to GDP ratio and banking sectorconcentration are significant determinants of bank efficiency. Besides, a modelwith risk-weighted assets as undesirable outputs can better capture the impactof shadow banking involvement.Keywords:Nonparametric Methods, Commercial Banks, Shadow Bank, Financial Risk
SEP
Abstract:The predictability of a high-dimensional time series model in forecasting withlarge information sets depends not only on the stability of parameters but alsodepends heavily on the active covariates in the model. Since the true empiricalenvironment can change as time goes by, the variables that function well at thepresent may become useless in the future. Combined with the instable parameters,finding the most active covariates in the parameter time-varying situationsbecomes difficult. In this paper, we aim to propose a new method, the PenalizedAdaptive Method (PAM), which can adaptively detect the parameter homogeneousintervals and simultaneously select the active variables in sparse models. Thenewly developed method is able to identify the parameters stability at one handand meanwhile, at the other hand, can manage of selecting the active forecastingcovariates at every different time point. Comparing with the classical models,the method can be applied to high-dimensional cases with different sources ofparameter changes while it steadily reduces the forecast error in high-dimensional data. In the out-of-sample bond risk premia forecasting, thePenalized Adaptive Method can reduce the forecasting error(RMSPE and MAPE)around 24% to 50% comparing with the other forecasting methods.Keywords:SCAD penalty, propagation-separation, adaptive window choice, multiplierbootstrap, bond risk premia
SEP
Abstract:The shift of human communication to online platforms brings many benefits tosociety due to the ease of publication of opinions, sharing experience, gettingimmediate feedback and the opportunity to discuss the hottest topics. Besidesthat, it builds up a space for antisocial behavior such as harassment, insultand hate speech. This research is dedicated to detection of antisocial onlinebehavior detection (AOB) - an umbrella term for cyberbullying, hate speech,cyberaggression and use of any hateful textual content. First, we provide abenchmark of deep learning models found in the literature on AOB detection. Deeplearning has already proved to be efficient in different types of decisionsupport: decision support from financial disclosures, predicting processbehavior, text-based emoticon recognition. We compare methods of traditionalmachine learning with deep learning, while applying important advancements ofnatural language processing: we examine bidirectional encoding, compareattention mechanisms with simpler reduction techniques, and investigate whetherthe hierarchical representation of the data and application of attention ondifferent layers might improve the predictive performance. As a partialcontribution of the final hierarchical part, we introduce pseudo-sentencehierarchical attention network, an extension of hierarchical attention network –a recent advancement in document classification.Keywords:Deep Learning, Cyberbullying, Antisocial Online Behavior, Attention Mechanism,Text Classification
SEP
Group Average Treatment Effects for Observational Studies   Daniel Jacob Wolfgang Karl Härdle Stefan Lessmann   Abstract: The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias we implement a doubly-robust estimator in the first stage. Keeping the flexibility, we can use any machine learning method to learn the conditional mean functions as well as the propensity score. We also use machine learning methods to learn a function for the conditional average treatment effect. The group average treatment effect, is then estimated via a parametric linear model to provide p-values and confidence intervals. To control for confounding in the linear model we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We introduce inclusion-probability weighting as a form of cross-splitting and averaging for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies. Keywords: causal inference, machine learning, simulation study, confidence intervals, multiple splitting, sorted group ATE (GATES), doubly-robust estimator 
SEP
Abstract:Public interest, explosive returns, and diversification opportunities gavestimulus to the adoption of traditional financial tools to crypto-currencies.While the CRIX index offered the first scientifically-backed proxy to thecrypto- market (analogous to S&P 500), the introduction of Bitcoin futures byCboe became the milestone in the creation of the derivatives market for crypto-currencies. Following the intuition of the "fear index" VIX for the Americanstock market, the VCRIX volatility index was created to capture the investorexpectations about the crypto-currency ecosystem. VCRIX is built based on CRIXand offers a forecast for the mean annualized volatility of the next 30 days,re-estimated daily. The model was back-tested for its forecasting power,resulting in low MSE performance and further examined by the simulation of VIX(resulting in a correlation of 78% between the actual VIX and VIX estimated withthe VCRIX model). VCRIX provides forecasting functionality and serves as a proxyfor the investors’ expectations in the absence of the de- veloped derivativesmarket. These features provide enhanced decision making capacities for marketmonitoring, trading strategies, and potentially option pricing.Keywords:index construction, volatility, crypto-currency, VCRIX
SEP
Abstract:Customer scoring models are the core of scalable direct marketing. Uplift modelsprovide an estimate of the incremental benefit from a treatment that is used foroperational decision-making. Training and monitoring of uplift models requireexperimental data. However, the collection of data under randomized treatmentassignment is costly, since random targeting deviates from an establishedtargeting policy. To increase the cost-efficiency of experimentation andfacilitate frequent data collection and model training, we introduce supervisedrandomization. It is a novel approach that integrates existing scoring modelsinto randomized trials to target relevant customers, while ensuring consistentestimates of treatment effects through correction for active sample selection.An empirical Monte Carlo study shows that data collection under supervisedrandomization is cost-efficient, while downstream uplift models performcompetitively.Keywords:Uplift Modeling, Causal Inference, Experimental Design, Selection Bias
SEP
Abstract:The integration of social media characteristics into an econometric frameworkrequires modeling a high dimensional dynamic network with dimensions ofparameter Θ typically much larger than the number of observations. To cope withthis problem, we introduce a new structural model — SONIC which assumes that(1) a few influencers drive the network dynamics; (2) the community structure ofthe network is characterized as the homogeneity of response to the specificinfluencer, implying their underlying similarity. An estimation procedure isproposed based on a greedy algorithm and LASSO regularization. Throughtheoretical study and simulations, we show that the matrix parameter can beestimated even when the observed time interval is smaller than the size of thenetwork. Using a novel dataset retrieved from a leading social media platform–StockTwits and quantifying their opinions via natural language processing, wemodel the opinions network dynamics among a select group of users and furtherdetect the latent communities. With a sparsity regularization, we can identifyimportant nodes in the network.Keywords:social media, network, community, opinion mining, natural language processing
SEP
Abstract:Among all the emerging markets, the cryptocurrency market is considered the mostcontroversial and simultaneously the most interesting one. The visiblysignificant market capitalization of cryptos motivates modern financialinstruments such as futures and options. Those will depend on the dynamics,volatility, or even the jumps of cryptos. In this paper, the riskcharacteristics for Bitcoin are analyzed from a realized volatility dynamicsview. The realized variance RV is estimated with (threshold-)jump components(T)J, semivariance RSV+(−) , and signed jumps (T)J+(−) . Our empirical resultsshow that the Bitcoin market is far riskier than any other developed financialmarket. Up to 68% of the sample days are identified to entangle jumps. However,the discontinuities do not contribute to the variance significantly. Byemploying a 90-day rolling-window method, the in-sample evidence suggests thatthe impacts of predictors change over time systematically under HAR-type models.The out-of-sample forecasting results reveal that the forecasting horizonplays an important role in choosing forecasting models. For long-horizon riskforecast, a finer model calibrated with jumps gives extra utility up to 20 basispoints annually, while an approach based on the roughest estimators suits theshort-horizon risk forecast best. Last but not least, a simple equal-weightedportfolio not only significantly reduces the size and quantity of jumps but alsogives investors higher utility in short-horizon risk forecast case.Keywords:Cryptocurrency, Bitcoin, Realized Variance, Thresholded Jump, Signed Jumps,Realized Utility
SEP
Abstract:The paper examines the potential of deep learning to produce decision supportmodels from structured, tabular data. Considering the context of financial riskmanagement, we develop a deep learning model for predicting whether individualspread traders are likely to secure profits from future trades. This embodiestypical modeling challenges faced in risk and behavior forecasting. Conventionalmachine learning requires data that is representative of the feature-targetrelationship and relies on the often costly development, maintenance, andrevision of handcrafted features. Consequently, modeling highly variable,heterogeneous patterns such as the behavior of traders is challenging. Deeplearning promises a remedy. Learning hierarchical distributed representations ofthe raw data in an automatic manner (e.g. risk taking behavior), it uncoversgenerative features that determine the target (e.g., trader’s profitability),avoids manual feature engineering, and is more robust toward change (e.g.dynamic market conditions). The results of employing a deep network foroperational risk forecasting confirm the feature learning capability of deeplearning, provide guidance on designing a suitable network architecture anddemonstrate the superiority of deep learning over machine learning and rule-based benchmarks.Keywords:risk management, retail finance, forecasting, deep learning
SEP
Abstract:The importance of startups for a dynamic, innovative and competitive economy hasalready been acknowledged in the scientific and business literature. The highlyuncertain and volatile nature of the startup ecosystem makes the evaluation ofstartup success through analysis and interpretation of information very timeconsuming and computationally intensive. This prediction problem brings forwardthe need for a quantitative model, which should enable an objective and fact-based approach to startup success prediction. This paper presents a series ofreproducible models for startup success prediction, using machine learningmethods. The data used for this purpose was received from the online investorplatform, crunchbase.com. The data has been pre-processed for sampling bias andimbalance by using the oversampling approach, ADASYN. A total of six differentmodels are implemented to predict startup success. Using goodness-of-fitmeasures, applicable to each model case, the best models selected are theensemble methods, random forest and extreme gradient boosting with a test setprediction accuracy of 94.1% and 94.5% and AUC of 92.22% and 92.91%respectively. Top variables in these models are last funding to date, firstfunding lag and company age. The models presented in this study can be used topredict success rate for future new firms/ventures in a repeatable way.Keywords:Machine learning
SEP
Abstract:A daily systemic risk measure is proposed accounting for links and mutualdependencies between financial institutions utilising tail event information. FRM(Financial Risk Meter) is based on Lasso quantile regression designed to capturetail event co-movements. The FRM focus lies on understanding active set datacharacteristics and the presentation of interdependencies in a network topology.Two FRM indices are presented, namely, FRM@Americas and FRM@Europe. The FRMindices detect systemic risk at selected areas and identifies risk factors. Inpractice, FRM is applied to the return time series of selected financialinstitutions and macroeconomic risk factors. Using FRM on a daily basis, weidentify companies exhibiting extreme "co-stress", as well as "activators" ofstress. With the SRM@EuroArea, we extend to the government bond asset class. FRMis a good predictor for recession probabilities, constituting the FRM-impliedrecession probabilities. Thereby, FRM indicates tail event behaviour in anetwork of financial risk factors.Keywords:Systemic Risk, Quantile Regression, Financial Markets, Risk Management, NetworkDynamics, Recession
SEP
Abstract:This research analyses high-frequency data of the cryptocurrency market inregards to intraday trading patterns. We study trading quantitatives such asreturns, traded volumes, volatility periodicity, and provide summary statisticsof return correlations to CRIX (CRyptocurrency IndeX), as well as respectiveoverall high-frequency based market statistics. Our results provide mandatoryinsight into a market, where the grand scale employment of automated tradingalgorithms and the extremely rapid execution of trades might seem to be astandard based on media reports. Our findings on intraday momentum of tradingpatterns lead to a new view on approaching the predictability of economic valuein this new digital market.Keywords:Cryptocurrency, High-Frequency Trading, Algorithmic Trading, Liquidity,Volatility, Price Impact, CRIX
SEP
Abstract:We propose an approach to calibrate the conditional value-at-risk (CoVaR) offinancial institutions based on neural network quantile regression. Building onthe estimation results we model systemic risk spillover effects across banks byconsidering the marginal effects of the quantile regression procedure. We adopt adropout regularization procedure to remedy the well-known issue of overfittingfor neural networks, and we provide empirical evidence for the favorable out-of-sample performance of a regularized neural network. We then propose threemeasures for systemic risk from our fitted results. We find that systemic riskincreases sharply during the height of the financial crisis in 2008 and againafter a short period of easing in 2011 and 2015. Our approach also allowsidentifying systemically relevant firms during the financial crisis.Keywords:Systemic risk, CoVaR, Quantile regression, Neural networks
SEP
Abstract:The aim of this paper is to prove the phenotypic convergence ofcryptocurrencies, in the sense that individual cryptocurrencies respond tosimilar selection pressures by developing similar characteristics. In order toretrieve the cryptocurrencies phenotype, we treat cryptocurrencies as financialinstruments (genus proximum) and find their specific difference (differentia specifica) by using the daily time series of log-returns. In this sense, a daily timeseries of asset returns (either cryptocurrencies or classical assets) can becharacterized by a multidimensional vector with statistical components likevolatility, skewness, kurtosis, tail probability, quantiles, conditional tailexpectation or fractal dimension. By using dimension reduction techniques(Factor Analysis) and classification models (Binary Logistic Regression,Discriminant Analysis, Support Vector Machines, K-means clustering, VarianceComponents Split methods) for a representative sample of cryptocurrencies,stocks, exchange rates and commodities, we are able to classify cryptocurrenciesas a new asset class with unique features in the tails of the log-returnsdistribution. The main result of our paper is the complete separation of thecryptocurrencies from the other type of assets, by using the Maximum VarianceComponents Split method. More, we observe a divergent evolution of thecryptocurrencies species, compared to the classical assets, mainly due to thetails behaviour of the log-returns distribution. The codes used here areavailable via www.quantlet.de.Keywords:cryptocurrency, genus proximum, differentia specifica, classification,multivariate analysis, factor models, phenotypic convergence, divergentevolution
SEP
Abstract:The paper presents a systematic theory for asymptotic inferences based onautocovariances of stationary processes. We consider nonparametric tests for serial correlations using the maximum and the quadratic deviations of sampleautocovariances. For these cases, with proper centering and rescaling, theasymptotic distributions of the deviations are Gumbel and Gaussian, respectively. To establish such an asymptotic theory, as byproducts, we develop anormal comparison principle and propose a sufficient condition for summabilityof joint cumulants of stationary processes. We adapt a blocks of blocksbootstrapping procedure proposed by Kuensch (1989) and Liu and Singh (1992) tothe maximum deviation based tests to improve the finite-sample performance.Keywords:Autocovariance, blocks of blocks bootstrapping, Box-Pierce test, extreme valuedistribution, moderate deviation, normal comparison, physical dependencemeasure, short range dependence, stationary process, summability of cumulants
SEP
Abstract:The 2017 bubble on the cryptocurrency market recalls our memory in the dot-combubble, during which hard-to-measure fundamentals and investors’ illusion forbrand new technologies led to overvalued prices. Benefiting from the massiveincrease in the volume of messages published on social media and message boards,we examine the impact of investor sentiment, conditional on bubble regimes, oncryptocurrencies aggregate return prediction. Constructing a crypto-specificlexicon and using a local-momentum autoregression model, we find that thesentiment effect is prolonged and sustained during the bubble while it turns outa reversal effect once the bubble collapsed. The out-of-sample analysis alongwith portfolio analysis is conducted in this study. When measuring investorsentiment for a new type of asset such as cryptocurrencies, we highlight thatthe impact of investor sentiment on cryptocurrency returns is conditional onbubble regimes.Keywords:Cryptocurrency; Sentiment; Bubble; Return Predictability
SEP
Abstract:We distill tone from a huge assortment of NASDAQ articles to examine thepredictive power of media-expressed tone in single-stock option markets andequity markets. We find that (1) option markets are impacted by media tone; (2)option variables predict stock returns along with tone; (3) option variablesorthogonalized to public information and tone are more effective predictors ofstock returns; (4) overnight tone appears to be more informative than trading-time tone, possibly due to a different thematic coverage of the trading versusthe overnight archive; (5) tone disagreement commands a strong positive riskpremium above and beyond market volatility.Keywords:option markets, equity markets, stock return predictability, media tone, topicmodel
SEP
Abstract:Increasingly volatile and distributed energy production challenge traditionalmechanisms to manage grid loads and price energy. Local energy markets (LEMs)may be a response to those challenges as they can balance energy production andconsumption locally and may lower energy costs for consumers. Blockchain-basedLEMs provide a decentralized market to local energy consumer and prosumers. Theyimplement a market mechanism in the form of a smart contract without the needfor a central authority coordinating the market. Recently proposed blockchain-based LEMs use auction designs to match future demand and supply. Thus, suchblockchain-based LEMs rely on accurate short-term forecasts of individualhouseholds’ energy consumption and production. Often, such accurate forecastsare simply assumed to be given. The present research tests this assumption.First, by evaluating the forecast accuracy achievable with state-of-the-artenergy forecasting techniques for individual households and, second, byassessing the effect of prediction errors on market outcomes in three differentsupply scenarios. The evaluation shows that, although a LASSO regression modelis capable of achieving reasonably low forecasting errors, the costly settlementof prediction errors can offset and even surpass the savings brought toconsumers by a blockchain-based LEM. This shows, that due to prediction errors,participation in LEMs may be uneconomical for consumers, and thus, has to betaken into consideration for pricing mechanisms in blockchain-based LEMs.Keywords:Blockchain; Local Energy Market; Smart Contract; Machine Learning; Household;Energy Prediction; Prediction Errors; Market Mechanism
SEP
Abstract:We consider a new procedure for detecting structural breaks in mean for high-dimensional time series. We target breaks happening at unknown time points andlocations. In particular, at a fixed time point our method is concerned witheither the biggest break in one location or aggregating simultaneous breaks overmultiple locations. We allow for both big or small sized breaks, so that we ca), stamp the dates and the locations of the breaks, 2), estimate the breaksizes and 3), make inference on the break sizes as well as the break dates. Ourtheoretical setup incorporates both temporal and crosssectional dependence, andis suitable for heavy-tailed innovations. We derive the asymptotic distributionfor the sizes of the breaks by extending the existing powerful theory on locallinear kernel estimation and high dimensional Gaussian approximation to allowfor trend stationary time series with jumps. A robust long-run covariance matrixestimation is proposed, which can be of independent interest. An application ondetecting structural changes of the US unemployment rate is considered toillustrate the usefulness of our method.Keywords:high-dimensional time series, multiple change-points, Gaussian approximation,nonparametric estimation, heavy tailed, long-run covariance matrix
SEP
Abstract:In this paper, we build an overlapping generation model to examine the reasonwhy developed countries with similar background have implemented differentsocial health insurance systems. We propose two hypotheses to explain thisphenomenon: (i) the different participation rates of the poor in the voting;(ii) the distinct attitudes towards the size of the government and the existenceof a compulsory social health insurance system. Agents need to vote for one oftwo policies: Policy I without Social Health Insurance (SHI) but with thesubsidy for the poor, and Policy II with fully covered SHI. By comparing eithertheir current utility or the expected life time utility, households will chooseone policy. We find that under Policy I, the derivative of the changes ofexpected utility with respect to income is not monotonic. This means that boththe poorest and the richest dislike the social health insurance system. With thecalibrated parameters, we solve the benchmark and find that the public’sattitude towards the size of the government and the lower representation of thepoor affect the election result. The changes in the minimum consumption levelunder Policy I affect the voting results most, followed by the attitude. VotingParticipant rate plays the most insignificant role in the voting outcome. Thesensitivity analysis shows that our main findings are robust to the inputparameters.Keywords:Social Health Insurance, Voting
SEP
Abstract:In this paper, we develop an multi period overlapping generation framework toinvestigate agents' consumption and saving decisions, inequality and welfareamong elderly. We assume that agents are heterogeneous in the non-asset incomeand the medical expenditure. In order to explicitly analyze the e ects ofmedical expenditure, we conduct three counterfactual exercises. We successivelyshut down the heterogeneity in labor income, in the level and in the dispersionof medical expenses respectively. By comparing the benchmark with thecounterfactual results, we  nd that in general wealth inequality decreases withage, and income uncertainty contributes the most to wealth inequality. Bothaverage consumption and consumption inequality increase with age. Consumptioninequality largely tracks income inequality. Though uncertainty in medicalexpenditures has little e ect on consumption inequality, a higher level ofmedical expenditures may exacerbate consumption inequality. Meanwhile, theaverage saving of elderly exhibits an inverse-U shape with age. The impacts onaverage saving are similar both in benchmark and in counterfactual exercises.Welfare increases with age.Keywords:Income Inequality, Social Mobility, Price-to-rent ratio
SEP
Abstract:Housing typically takes up a major proportion of households' expenditure, andthus it certainly plays a critical role in shaping the pattern of income in-equality and social mobility. Whether high housing price-to-rent ratio will am-plify inequality and inhibit social class upgrading is still a controversialissue in the existing literature. In this paper, we develop a partialequilibrium life- cycle framework to address these issues. Agents in our economyare divided into two social classes according to the initial human capital levelinherited from their parents. Those who belong to upper class will draw theirinnate abilities from a distribution that  rst order stochastically dominatesthose from lower class. Throughout the entire lifecycle, agents make endogenoushuman capital investment and housing tenure decisions. We calibrate the model tomimic some stylized facts in the the real world counter part. Our simulationresults indicate an inverse-U pattern between housing price-to-rent ratio andmeasures of income inequality, and as well as a U-shape pattern between price-to-rent ratio and social mobility measured by Shorrocks Index. The implicationis that housing tends to amplify the inequality and slow down the socialmobility when houses can only be purchased by a small group of agents in theeconomy. Moreover, our results also suggest that better quality of education asa result of a higher return to human capital investment tends to dampen the roleof housing.Keywords:Income Inequality, Social Mobility, Price-to-rent ratio
SEP
Abstract:Cryptocurrencies are becoming an attractive asset class and are the focus ofrecent quantitative research. The joint dynamics of the cryptocurrency marketyields information on network risk. Utilizing the adaptive LASSO approach, webuild a dynamic network of cryptocurrencies and model the latent communitieswith a dynamic stochastic blockmodel. We develop a dynamic covariate-assistedspectral clustering method to uniformly estimate the latent group membership ofcryptocurrencies consistently. We show that return inter-predictability andcrypto characteristics, including hashing algorithms and proof types, jointlydetermine the crypto market segmentation. Based on this classification result,it is natural to employ eigenvector centrality to identify a cryptocurrency’sidiosyncratic risk. An asset pricing analysis finds that a cross-sectionalportfolio with a higher centrality earns a higher risk premium. Further testsconfirm that centrality serves as a risk factor well and delivers valuableinformation content on cryptocurrency markets.Keywords:Community Detection, Dynamic Stochastic Blockmodel, Spectral Clustering, NodeCovariate, Return Predictability, Portfolio Management
SEP
Abstract:Deep learning has substantially advanced the state-of-the-art in computervision, natural language processing and other  elds. The paper examines thepotential of contemporary recurrent deep learning architectures for  nancialtime series forecasting. Considering the foreign exchange market as testbed, wesystematically compare long short-term memory networks and gated recurrent unitsto traditional recurrent architectures as well as feedforward networks in termsof their directional forecasting accuracy and the profitability of trading modelpredictions. Empirical results indicate the suitability of deep networks forexchange rate forecasting in general but also evidence the diculty ofimplementing and tuning corresponding architectures. Especially with regard totrading pro t, a simpler neural network may perform as well as if not betterthan a more complex deep neural network.Keywords:Deep learning, Financial time series forecasting, Recurrent neural networks,Foreign exchange rates
SEP
Abstract:Risk transmission among financial markets and their participants is time-evolving, especially for the extreme risk scenarios. Possibly sudden timevariation of such risk structures ask for quantitative technology that is ableto cope with such situations. Here we present a novel localized multivariateCAViaR-type model to respond to the challenge of time-varying risk contagion.For this purpose a local adaptive approach determines homogeneous, low riskvariation intervals at each time point. Critical values for this technique arecalculated via multiplier bootstrap, and the statistical properties of this“localized multivariate CAViaR” are derived. A comprehensive simulation studysupports the effectiveness of our approach in detecting structural change inmultivariate CAViaR. Finally, when applying for the US and German financialmarkets, we can trace out the dynamic tail risk spillovers and find that the USmarket appears to play dominate role in risk transmissions, especially involatile market periods.Keywords:conditional quantile autoregression, local parametric approach, change pointdetection, multiplier bootstrap
SEP
Abstract:Understanding the topological structure of real world networks is of hugeinterest in a variety of fields. One of the way to investigate this structure isto find the groups of densely connected nodes called communities. This paperpresents a new non-parametric method of community detection in networks calledAdaptive Weights Community Detection. The idea of the algorithm is to associatea local community for each node. On every iteration the algorithm tests ahypothesis that two nodes are in the same community by comparing their localcommunities. The test rejects the hypothesis if the density of edges betweenthese two local communities is lower than the density inside each one. Adetailed performance analysis of the method shows its dominance over state-of-the-art methods on well known artificial and real world benchmarks.Keywords:Adaptive weights, Gap coefficient, Graph clustering, Nonparametric, Overlappingcommunities
SEP
Abstract:Software-as-a-service applications are experiencing immense growth as theircomparatively low cost makes them an important alternative to traditionalsoftware. Following the initial adoption phase, vendors are now concerned withthe continued usage of their software. To analyze the influence of differentmeasures to improve continued usage over time, a longitudinal study design usingdata from a SaaS vendor was implemented. By employing a linear mixed model, thestudy finds several measures to have a positive effect on a software’s usagepenetration. In addition to these activation measures performed by the SaaSvendor, software as well as client characteristics were likewise examined butdid not display significant estimates. In summary the study contributes novelinsights into the scarcely researched field of influencing factors on SaaS usagecontinuance.Keywords:Linear Mixed Models Software-as-a-Service Usage Continuance
SEP
Abstract:This paper provides a detailed framework for modeling portfolios, achieving thehighest growth rate under subjective risk constraints such as Value at Risk(VaR) in the presence of stable laws. Although the maximization of the expectedlogarithm of wealth induces outperforming any other significantly differentstrategy, the Kelly Criterion implies larger bets than a risk-averse investorwould accept. Restricting the Kelly optimization by spectral risk measures, theauthors provide a generalized mapping for different measures of growth andsecurity. Analyzing over 30 years of S&P 500 returns for different samplingfrequencies, the authors find evidence for leptokurtic behavior for allrespective sampling frequencies. Given that lower sampling frequencies imply asmaller number of data points, this paper argues in favor of α-stable laws andits scaling behavior to model financial market returns for a given horizon in ani.i.d. world. Instead of simulating from the class of elliptically stabledistributions, a nonparametric scaling approximation, based on the data-setitself, is proposed. Our paper also uncovers that including long put optionsinto the portfolio optimization, improves the growth criterion for a givensecurity level, leading to a new Kelly portfolio providing the highest geometricmean.Keywords:growth-optimal, Kelly criterion, protective put, portfolio optimization, stabledistribution, Value at Risk
SEP
Abstract:Weekly, quarterly and yearly risk measures are crucial for risk reportingaccording to Basel III and Solvency II. For the respective data frequencies, theauthors show in a simulation and backtest study that available data series arenot sufficient in order to estimate Value at Risk and Expected Shortfallsufficiently, given confidence levels of 99.9% and 99.99%. Accordingly, thispaper presents a semi-parametric estimation method, rescaling data from high- tolow-frequency which allows to obtain significantly more data points for theestimation of the respective risk measures. The presented methodology in theα-stable framework, which is able to mimic multifractal behavior in assetreturns, provides tail events which never occurred in the original low-frequencydataset.Keywords:high-frequency, multifractal, stable distribution, rescaling, risk management,Value at Risk, quantile distribution
SEP
Abstract:This work aims to investigate the (inter)relations of information arrival, newssentiment, volatilities and jump dynamics of intraday returns. Two parametricGARCH-type jump models which explicitly incorporate both news arrival and newssentiment variables are proposed, among which one assumes news affectingfinancial markets through the jump component while the other postulating theGARCH component channel. In order to give the most-likely format of theinteractions between news arrival and stock market behaviors, these two modelsare compared with several other easier versions of GARCH-type models based onthe calibration results on DJIA 30 stocks. The necessity to include newsprocesses in intraday stock volatility modeling is justified in our specificcalibration samples (2008 and 2013, respectively). While it is not as profitableto model jump process separately as using simpler GARCH process with errordistribution capable to capture fat tail behaviors of financial time series. Inconclusion, our calibration results suggest GARCH-news model with skew-tinnovation distribution as the best candidate for intraday returns of largestocks in US market, which means one can probably avoid the complicatedness ofmodelling jump behavior by using a simplier skew-t error distribution assumptioninstead, but it’s necessary to incorporate news variables.Keywords:information arrival, volatility modeling, jump, sentiment, GARCH
SEP
Abstract:Excessive house price growth was at the heart of the financial crisis i07/08. Since then, many countries have added cooling measures to theirregulatory frameworks. It has been found that these measures can indeed controlprice growth, but no one has examined whether this has adverse consequences forthe housing wealth distribution. We examine this for Singapore, which started i09 to target price growth over ten rounds in total. We find that welfare fromhousing wealth in the last round might not be higher than before 2009. Thisdepends on the deflator used to convert nominal into real prices. Irrespectiveof the deflator, we can reject that welfare increased monotonically over thedifferent rounds.Keywords:house price distribution, stochastic dominance tests
SEP
Abstract:We study investor sentiment on a non-classical asset, cryptocurrencies using a“cryptospecificlexicon” recently proposed in Chen et al. (2018) and statisticallearning methods.We account for context-specific information and word similarityby learning word embeddingsvia neural network-based WorVec model. On top ofpre-trained word vectors, weapply popular machine learning methods such asrecursive neural networks for sentencelevelclassification and sentiment indexconstruction. We perform this analysis on a noveldataset of 1220K messagesrelated to 425 cryptocurrencies posted on a microblogging platformStockTwitsduring the period between March 2013 and May 2018. The constructed sentimentindices are value-relevant in terms of its return and volatility predictabilityfor thecryptocurrency market index.Keywords:sentiment analysis, lexicon, social media, word embedding, deep learning
SEP
Abstract:Second-hand car markets contribute to billions of Euro turnover each year buthardly generate profit for used car dealers. The paper examines the potential ofsophisticated data-driven pricing systems to enhance supplier-side decision-making and escape the zero-profit-trap. Profit maximization requires an accurateunderstanding of demand. The paper identifies factors that characterize consumerdemand and proposes a framework to estimate demand functions using survivalanalysis. Empirical analysis of a large data set of daily used car sales betwee08 to 2012 confirm the merit of the new factors. Observed results also showthe value of survival analysis to explain and predict demand. Random survivalforest emerges as the most suitable vehicle to develop price response functionsas input for a dynamic pricing system.Keywords:Automotive Industry, Price Optimization, Survival Analysis, Dynamic Pricing
SEP
Abstract: A copula model with flexibly specified dependence structure can be useful to capture the complexity and heterogeneity in economic and financial time series. However, there exists little methodological guidance for the specification process using copulas. This paper contributes to fill this gap by considering the recently proposed single-index copulas, for which we propose a simultaneous estimation and variable selection procedure. The proposed method allows to choose the most relevant state variables from a comprehensive set using a penalized estimation, and we derive its large sample properties. Simulation results demonstrate the good performance of the proposed method in selecting the appropriate state variables and estimating the unknown index coefficients and dependence parameters. An application of the new procedure identifies six macroeconomic driving factors for the dependence among U.S. housing markets.  Keywords:Semiparametric Copula, Single-Index Copula, Variable Selection, SCAD  
SEP
Abstract Uplift modeling combines machine learning and experimental strategies to estimate the differential effect of a treatment on individuals behavior. The paper considers uplift models in the scope of marketing campaign targeting. Literature on uplift modeling strategies is fragmented across academic disciplines and lacks an overarching empirical comparison. Using data from online retailers, we fill this gap and contribute to literature through consolidating prior work on uplift modeling and systematically comparing the predictive performance and utility of available uplift modeling strategies. Our empirical study includes three experiments in which we examine the interaction between an uplift modeling strategy and the underlying machine learning algorithm to implement the strategy, quantify model performance in terms of business value and demonstrate the advantages of uplift models over response models, which are widely used in marketing. The results facilitate making specific recommendations how to deploy uplift models in e-commerce applications.  Keywords:  e-commerce analytics, machine learning, uplift modeling, real-time targeting  
SEP
Abstract In deconvolution in Rd; d  1; with mixing density p(2 P) and kernel h; the mixture density fp(2 Fp) can always be estimated with f^pn; ^pn 2 P; via Minimum Distance Estimation approaches proposed herein, with calculation of f^pn's upper -error rate, an; in probability or in risk; h is either known or unknown, an decreases to zero with n: In applications, an is obtained when P consists either of products of d densities dened on a compact, or  separable densities in R with their dierences changing sign at most J times; J is either known or unknown. When h is known and p is ~q-smooth, vanishing outside a compact in Rd; plug-in upper bounds are then also provided for the -error rate of ^pn and its derivatives, respectively, in probability or in risk; ~q 2 R+; d  1: These -upper bounds depend on h's Fourier transform, ~h(6= 0); and have rates (log a??1 n )?? and a n , respectively, for h super-smooth and smooth;  > 0;  > 0: For the typical an  (log n)  n??; the former (logarithmic) rate bound is optimal for any  > 0 and the latter misses the optimal rate by the factor (log n) when  = :5;  > 0;  > 0: The exponents  and  appear also in optimal rates and lower error and risk bounds in the deconvolution literature.  Keywords:    
SEP
Abstract In linear regression of Y on X(2 Rp) with parameters (2 Rp+1); statistical inference is unreliable when observations are obtained from gross-error model, F;G = (1??)F +G; instead of the assumed probability F;G is gross-error probability, 0 <  < 1: When G is unit mass at (x; y); Residual's In uence Index, RINFIN(x; y; ; ), measures the dierence in small x-perturbations of -residual, r(x; y); for model F and for F;G via r's x-partial derivatives. Asymptotic properties are presented for sample RINFIN that is successful in extracting indications for in uential and bad leverage cases in microarray data and simulated, high dimensional data. Its performance improves as p increases and can also be used in multiple response linear regression. RINFIN's advantage is that, whereas in in uence functions of -regression coecients each x-coordinate and r(x; y) appear in a sum as product with moderate size when (x; y) is bad leverage case and masking makes r(x; y) nearly vanish, RINFIN's x-partial derivatives convert the product in sum allowing for unmasking.  Keywords:  Big Data, Data Science, In fluence Function, Leverage, Masking, Residual's In fluence Index (RINFIN)  
SEP
Abstract High-dimensional, streaming datasets are ubiquitous in modern applications. Examples range from nance and e-commerce to the study of biomedical and neuroimaging data. As a result, many novel algorithms have been proposed to address challenges posed by such datasets. In this work, we focus on the use of - regularized linear models in the context of (possibly non-stationary) streaming data. Recently, it has been noted that the choice of the regularization parameter is fundamental in such models and several methods have been proposed which iteratively tune such a parameter in a time-varying manner, thereby allowing the underlying sparsity of estimated models to vary. Moreover, in many applications, inference on the regularization parameter may itself be of interest, as such a parameter is related to the underlying sparsity of the model. However, in this work, we highlight and provide extensive empirical evidence regarding how various (often unrelated) statistical properties in the data can lead to changes in the regularization parameter. In particular, through various synthetic experiments, we demonstrate that changes in the regularization parameter may be driven by changes in the true underlying sparsity, signal-to-noise ratio or even model misspecication. The purpose of this letter is, therefore, to highlight and catalog various statistical properties which induce changes in the associated regularization parameter. We conclude by presenting two applications: one relating to nancial data and another to neuroimaging data, where the aforementioned discussion is relevant.  Keywords:  Lasso, penalty parameter, stock prices, neuroimaging  
SEP
Abstract The market capitalization of cryptocurrencies has risen rapidly during the last few years. Despite their high volatility, this fact has spurred growing interest in cryptocurrencies as an alternative investment asset for portfolio and risk management. We characterise the effects of adding cryptocurrencies in addition to traditional assets to the set of eligible assets in portfolio management. Out-of-sample performance and diversification benefits are studied for the most popular portfolio-construction rules, including mean-variance optimization, risk-parity, and maximum-diversification strategies, as well as combined strategies.  To account for the frequently low liquidity of cryptocurrency markets we incorporate the LIBRO method, which gives suitable liquidity constraints. Our results show that cryptocurrencies can improve the risk-return profile of portfolios. In particular, cryptocurrencies are more useful for portfolio strategies with higher target returns; they do not play a role in minimum-variance portfolios. However, a maximum-diversification strategy (maximising the Portfolio Diversification Index, PDI) draws appreciably on cryptocurrencies, and spanning tests clearly indicate that cryptocurrency returns are non-redundant additions to the investment universe.  Keywords:  cryptocurrency, CRIX, investments, portfolio management, asset classes, blockchain, Bitcoin, altcoins, DLT  
SEP
Abstract Modeling the joint tails of multiple nancial time series has important im- plications for risk management. Classical models for dependence often encounter a lack of t in the joint tails, calling for additional  exibility. In this paper we introduce a new nonparametric time-varying mixture copula model, in which both weights and depen- dence parameters are deterministic functions of time. We propose penalized trending mixture copula models with group smoothly clipped absolute deviation (SCAD) penal- ty functions to do the estimation and copula selection simultaneously. Monte Carlo simulation results suggest that the shrinkage estimation procedure performs well in s- electing and estimating both constant and trending mixture copula models. Using the proposed model and method, we analyze the evolution of the dependence among four international stock markets, and nd substantial changes in the levels and patterns of the dependence, in particular around crisis periods.  Keywords:  Copula, Time-Varying Copula, Mixture Copula, Copula Selection  
SEP
Abstract In this paper we investigate the statistical properties of cryptocurrencies by using alpha-stable distributions. We also study the benefits of the Metcalfe's law (the value of a network is proportional to the square of the number of connected users of the system) for the evaluation of cryptocurrencies. As the results showed a potential for herding behaviour, we used LPPL models to capture the behaviour of cryptocurrencies exchange rates during an endogenous bubble and to predict the most probable time of the regime switching.  Keywords:  cryptocurrency, Bitcoin, CRIX, Log-Periodic Power Law, Metcalfes law, stable distribution  
SEP
Abstract An extensive empirical literature documents a generally negative relation, named the leverage effect, between asset returns and changes of volatility. It is more challenging to establish such a return-volatility relationship for jumps in high-frequency data. We propose new nonparametric methods to assess and test for a discontinuous leverage effect  i.e. a covariation between contemporaneous jumps in prices and volatility. The methods are robust to market microstructure noise and build on a newly developed price-jump localization and estimation procedure. Our empirical investigation of six years of transaction data from 320 NASDAQ firms displays no unconditional negative covariation between price and volatility cojumps. We show, however, that there is a strong and significant discontinuous leverage effect if one conditions on the sign of price jumps and whether the price jumps are market-wide or idiosyncratic.  Keywords:  High-frequency data, market microstructure, news impact, market-wide jumps, price jump, volatility jump  
SEP
Abstract Open-ended responses are widely used in market research studies. Processing of such responses requires labor-intensive human coding. This paper focuses on unsupervised topic models and tests their ability to automate the analysis of open-ended responses. Since state-ofthe- art topic models struggle with the shortness of open-ended responses, the paper considers three novel short text topic models: Latent Feature Latent Dirichlet Allocation, Biterm Topic Model and Word Network Topic Model. The models are fitted and evaluated on a set of realworld open-ended responses provided by a market research company. Multiple components such as topic coherence and document classification are quantitatively and qualitatively evaluated to appraise whether topic models can replace human coding. The results suggest that topic models are a viable alternative for open-ended response coding. However, their usefulness is limited when a correct one-to-one mapping of responses and topics or the exact topic distribution is needed.  Keywords:  Market research, open-ended responses, text analytics, short text topic models  
SEP
Abstract This paper studies the short-run impacts of temperature on human performance in the computer-mediated environment using server logs of a popular online game in China. Taking advantage of the quasi-experiment of winter central heating policy inChina, we distinguish the impacts of outdoor and indoor temperature and find that low temperatures below 5 ?C decrease game performance significantly. Non-experienced players suffered larger performance drop than experienced ones. Access to central heating attenuates negative impacts of low outdoor temperatures on gamers performance. High temperatures above 21 ?C also lead to drops in game performance.We conclude that expanding the current central heating zone will bring an increase in human performance by approximately 4% in Shanghai and surrounding provinces in the winter. While often perceived as a leisure activity, online gaming requires intense engagement and the deployment of cognitive, social, and motor skills, which are also key skills for productive activities. Our results draw attention to potential damages of extreme temperature on human performance in the modern computer-mediated environment.  Keywords:  Temperature, Human performance, Online game, Heating  
SEP
Abstract In this article, we study a nonparametric approach regarding a general nonlinear reduced form equation to achieve a better approximation of the optimal instrument. Accordingly, we propose the nonparametric additive instrumental variable estimator (NAIVE) with the adaptive group Lasso.We theoretically demonstrate that the proposed estimator is root-n consistent and asymptotically normal. The adaptive group Lasso helps us select the valid instruments while the dimensionality of potential instrumental variables is allowed to be greater than the sample size. In practice, the degree and knots of B-spline series are selected by minimizing the BIC or EBIC criteria for each nonparametric additive component in the reduced form equation. In Monte Carlo simulations, we show that the NAIVE has the same performance as the linear instrumental variable (IV) estimator for the truly linear reduced form equation. On the other hand, the NAIVE performs much better in terms of bias and mean squared errors compared to other alternative estimators under the high-dimensional nonlinear reduced form equation. We further illustrate our method in an empirical study of international trade and growth. Our findings provide  Keywords:  Adaptive group Lasso; Instrumental variables; Nonparametric additive model; Optimal estimator; Variable selection.  
SEP
Abstract The conventional wisdom that housing prices are the present value of future rents ignores the fact that unlike dividends on stocks, rent is not discretionary. Housing price uncertainty can affect household property investments, which in turn affect rent. By extending the theory of investment under uncertainty, we model the renters decision to buy a house and the landlords decision to sell as the exercising of real options of waiting and examine real options effects on rent. Using data from Hong Kong and mainland China, we find a significant effect of housing price on rent and draw important policy implications.  Keywords:    
SEP
Abstract This paper is concerned with selecting important covariates and estimating the index direction simultaneously for high dimensional single-index models. We develop an efficient Threshold Gradient Directed Regularization method via maximizing Distance Covariance (DC-TGDR) between the single index and response variable. Due to the appealing property of distance covariance which can measure nonlinear dependence between random variables, the proposed method avoids estimating the unknown link function of the single index and dramatically reduces computational complexity compared to other methods that use smoothing techniques. It keeps the model-free advantage from the view of sufficient dimension reduction and requires neither predictors nor response variable to be continuous. In addition, the DC-TGDR method encourages a grouping effect. That is, it is capable of choosing highly correlated covariates in or out of the model together. We examine finite-sample performance of the proposed method by Monte Carlo simulations. In a real data analysis, we identify important copy number alterations (CNAs) for gene expression.  Keywords:  Distance covariance, Highdimensional data, Threshold gradient directed regularization, Single-index models, Variable selection.  
SEP
Abstract In this article we develop a tractable procedure for testing strict stationarity in a double autoregressive model and formulate the problem as testing if the top Lyapunov exponent is negative. Without strict stationarity assumption, we construct a consistent estimator of the associated top Lyapunov exponent and employ a random weighting approach for its variance estimation, which in turn are used in a t-type test. We also propose a GLAD estimation for parameters of interest, relaxing key assumptions on the commonly used QMLE. All estimators, except for the intercept, are shown to be consistent and asymptotically normal in both stationary and explosive situations. The nite-sample performance of the proposed procedures is evaluated via Monte Carlo simulation studies and a real dataset of interest rates is analyzed.  Keywords:  DAR model, GLAD estimation, Nonstationarity, Random weighting, Strict stationarity testing.  
SEP
Abstract In this paper, we propose a new class of regime shift models with flexible switching mechanism that relies on a nonparametric probability function of the observed threshold variables. The proposed models generally embrace traditional threshold models with contaminated threshold variables or heterogeneous threshold values, thus gaining more power in handling complicated data structure. We solve the identification issue by imposing either global shape restriction or boundary condition on the nonparametric probability function. We utilize the natural connection between penalized splines and hierarchical Bayes to conduct smoothing. By adopting different priors, our procedure could work well for estimations of smooth curve as well as discontinuous curves with occasionally structural breaks. Bayesian tests for the existence of threshold effects are also conducted based on the posterior samples from Markov chain Monte Carlo (MCMC) methods. Both simulation studies and an empirical application in predicting the U.S. stock market returns demonstrate the validity of our methods.  Keywords:  Threshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian Inference, Spline.  
SEP
Abstract In this article, we propose a new class of semiparametric instrumental variable models with partially varying coefficients, in which the structural function has a partially linear form and the impact of endogenous structural variables can vary over different levels of some exogenous variables. We propose a three-step estimation procedure to estimate both functional and constant coefficients. The consistency and asymptotic normality of these proposed estimators are established. Moreover, a generalized F-test is developed to test whether the functional coefficients are of particular parametric forms with some underlying economic intuitions, and furthermore, the limiting distribution of the proposed generalized F-test statistic under the null hypothesis is established. Finally, we illustrate the finite sample performance of our approach with simulations and two real data examples in economics.  Keywords:  Endogeneity; Functional coefficients; Generalized F-test; Instrumental variables models; Nonparametric test; Profile least squares  
SEP
Abstract We model the term structure of implied volatility (TSIV) with an adaptive approach to improve predictability, which treats dynamic time series models of globally time- varying but locally constant parameters and uses a data-driven procedure to ?nd the local optimal interval. We choose two speci?cations of the adaptive models: a simple local AR (LAR) model for a univariate implied volatility series and an adaptive dynamic Nelson-Siegel (ADNS) model of three factors, each based on an LAR, to model the cross- section of the TSIV simultaneously with parsimony. Both LAR and ADNS models uniformly outperform more than a dozen alternative models with signi?cance across maturities for 1-20 day forecast horizons. Measured by RMSE and MAE, the forecast errors of the random walk model can be reduced by between 20% and 60% for the 5 to 20 days ahead forecast. In terms of prediction accuracy of future directional changes, the adaptive models achieve an accuracy range of 60%-90%, which strictly dominates the range of 30%-59% of the alternative models.  Keywords:  Term structure of implied volatility, local parametric models, forecasting  
SEP
Abstract A trading rule that draws on the empirical similarity concept is proposed to simulate the technical trading mentality|one that selectively perceives structural resemblances between market scenarios of the present and the past. In more than half of the nineteen futures markets that we test against for protability of this similarity-based trading rule, we nd evidence of predictive ability that is robust to data-snooping and transaction-cost adjust- ments. When aided by an exit strategy that liquidates the trader's positions across some evenly-spaced time points, this rule generates the most robust returns.  Keywords:  empirical similarity; technical trading; futures markets; analogical reasoning  
SEP
Abstract Cryptocurrencies refer to a type of digital cash that use distributed ledger - or blockchain technology - to provide secure transactions. These currencies are generally misunderstood. While initially dismissed as fads or bubbles, many large central banks are considering launching their own version of national cryptocurrencies. In contrast to most data in nancial economics, there is a plethora of detailed (free) data on the history of every transaction for the cryptocurrency complex. Further, there is little empirically-oriented research on this new asset class. This is an extraordinary research opportunity for academia. We provide a starting point by giving an insight into cryptocurrency mechanisms and detailing summary statistics and focusing on potential future research avenues in nancial economics.  Keywords:  Cryptocurrency, Blockchain, Bitcoin, Economic bubbles, Peer-to-Peer, Cryptographic hashing, Consensus, Proof-of-Work, Proof-of-stake, Volatility  
SEP
Abstract News move markets and contains incremental information about stock reactions. Future trading volumes, volatility and returns are a ected by sentiments of texts and opinions expressed in articles. Earlier work of sentiment distillation of stock news suggests that risk prole reactions might differ across sectors. Conventional asset pricing theory recognizes the role of a sector and its risk uniqueness that differs from market or rm specic risk. Our research assesses whether incorporating the sentiment distilled from sector specic news carries information about risk proles. Textual analytics applied to about 600K articles leads us with lexical projection and machine learning to classication of sentiment polarities. The texts are scraped from offcial NASDAQ web pages and with Natural Language Processing (NLP) techniques, such as tokenization, lemmatization, a sector specic sentiment is extracted using a lexical approach and a nancial phrase bank. Predicted sentence-level polarities are aggregated into a bullishness measure on a daily basis and fed into a panel regression analysis with sector indicators. Supervised learning with hinge or logistic loss and regularization yields good prediction results of polarity. Compared with standard lexical projections, the supervised learning approach yields superior predictions of sentiment, leading to highly sector specic sentiment reactions. The Consumer Staples, Health Care and Materials sectors show strong risk prole reactions to negative polarity.  Keywords:  Investor Sentiment, Attention Analysis, Sector-specic Reactions, Volatility, Text Mining, Polarity  
SEP
Abstract In this paper, the complete convergence for maximal weighted sums of extended negatively dependent (END, for short) random variables is investigated. Some sucient conditions for the complete convergence and some applications to a nonparametric model are provided. The results obtained in the paper generalise and improve the corresponding ones of Wang el al. (2014b) and Shen, Xue, and Wang (2017).  Keywords:  Complete convergence; Maximal weighted sums; Extended negatively dependent.  
SEP
Abstract We consider a generalization of Baum-Katz theorem for random vari- ables satisfying some cover conditions. Consequently, we get the result for many dependent structure, such as END, -mixing, -mixing and -mixing, etc.  Keywords:  Complete convergence; Marcinkiewicz-Zygmund type SLLN; Extended negatively dependent; Mixing dependency; Weakly mean bounded.  
SEP
Abstract In this paper, the complete convergence and complete moment convergence for maximal weighted sums of extended negatively dependent random variables are investigated. Some su±cient conditions for the convergence are provided. In addition, the Marcinkiewicz{Zygmund type strong law of large numbers for weighted sums of extended negatively dependent random variables is obtained. The results obtained in the article extend the corresponding ones for independent random variables and some dependent random variables.  Keywords:  Extended negatively dependent, complete convergence, complete moment convergence, maximal weighted sums, strong law of large numbers  
SEP
Abstract In the present paper we propose a new method, the Penalized Adaptive Method (PAM), for a data driven detection of structural changes in sparse linear models. The method is able to allocate the longest homogeneous intervals over the data sample and simultaneously choose the most proper variables with the help of penalized regression models. The method is simple yet  exible and can be safely applied in high-dimensional cases with dierent sources of parameter changes. Comparing with the adaptive method in linear models, its combination with dimension reduction yields a method which properly selects signicant variables and detects structural breaks while steadily reduces the forecast error in high-dimensional data.  Keywords:  SCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap  
SEP
Abstract Starting from well-known empirical stylised facts of nancial time series, we develop dynamic portfolio protection trading strategies based on econometric methods. As a criterion for riskiness we consider the evolution of the value-at-risk spread from a GARCH model with normal innovations relative to a GARCH model with generalised innovations. These generalised innovations may for example follow a Student t, a generalised hyperbolic (GH), an alpha-stable or a Generalised Pareto (GPD) distribution. Our results indicate that the GPD distribution provides the strongest signals for avoiding tail risks. This is not surprising as the GPD distribution arises as a limit of tail behaviour in extreme value theory and therefore is especially suited to deal with tail risks. Out-of-sample backtests on 11 years of DAX futures data, indicate that the dynamic tail-risk protection strategy eectively reduces the tail risk while outperforming traditional portfolio protection strategies. The results are further validated by calculating the statistical signicance of the results obtained using bootstrap methods. A number of robustness tests including application to other assets further underline the eectiveness of the strategy. Finally, by empirically testing for second order stochastic dominance, we nd that risk averse investors would be willing to pay a positive premium to move from a static buy-and-hold investment in the DAX future to the tail-risk protection strategy.  Keywords:  tail-risk protection, portfolio protection, extreme events, tail distributions  
SEP
Abstract We investigate default probabilities and default correlations of Merton-type credit portfolio models in stress scenarios where a common risk factor is truncated. The analysis is performed in the class of elliptical distributions, a family of light-tailed to heavy-tailed distributions encompassing many distributions commonly found in nancial modelling. It turns out that the asymptotic limit of default probabilities and default correlations depend on the max-domain of the elliptical distribution's mixing variable. In case the mixing variable is regularly varying, default probabilities are strictly smaller than 1 and default correlations are in (0; 1). Both can be expressed in terms of the Student t-distribution function. In the rapidly varying case, default probabilities are 1 and default correlations are 0. We compare our results to the tail dependence function and discuss implications for credit portfolio modelling.   Keywords:  financial risk management, credit portfolio modelling, stress testing, elliptic distribution, max-domain  MSC classification:  60, 91
SEP
Abstract Paralleling regulatory developments, we devise value-at-risk and expected shortfall type risk measures for the potential losses arising from using misspecied models when pricing and hedging contingent claims. Essentially, losses from model risk correspond to losses realized on a perfectly hedged position. Model uncertainty is expressed by a set of pricing models, relative to which potential losses are determined. Using market data, a unied loss distribution is attained by weighing models according to a relative likelihood criterion. Examples demonstrate the magnitude of model risk and corresponding capital buers necessary to suciently protect trading book positions against unexpected losses from model risk.   Keywords:  Model risk, parameter uncertainty, hedge error, value-at-risk, expected shortfall  JEL Clasification: , 
SEP
Abstract We investigate correlations of asset returns in stress scenarios where a common risk factor is truncated. Our analysis is performed in the class of normal variance mixture (NVM) models, which encompasses many distributions commonly used in nancial modelling. For the special cases of jointly normally and t-distributed asset returns we derive closed formulas for the correlation under stress. For the NVM distribution, we calculate the asymptotic limit of the correlation under stress, which depends on whether the variables are in the maximum domain of attraction of the Frechet or Gumbel distribution. It turns out that correlations in heavy-tailed NVM models are less sensitive to stress than in medium- or light-tailed models. Our analysis sheds light on the suitability of this model class to serve as a quantitative framework for stress testing, and as such provides valuable information for risk and capital management in nancial institutions, where NVM models are frequently used for assessing capital adequacy. We also demonstrate how our results can be applied for more prudent stress testing.   Keywords:  Stress testing, risk management, correlation, normal variance mixture distribution, multivariate normal distribution, multivariate t-distribution.
SEP
Abstract In 2012, JPMorgan accumulated a USD 6.2 billion loss on a credit derivatives portfolio, the so-called \London Whale", partly as a consequence of de-correlations of non-perfectly correlated positions that were supposed to hedge each other. Motivated by this case, we devise a factor model for correlations that allows for scenario-based stress-testing of correlations. We derive a number of analytical results related to a portfolio of homogeneous assets. Using the concept of Mahalanobis distance, we show how to identify adverse scenarios of correlation risk. As an example, we apply the factor-model approach to the \London Whale" portfolio and determine the value-at-risk impact from correlation changes. Since our ndings are particularly relevant for large portfolios, where even small correlation changes can have a large impact, a further application would be to stress-test portfolios of central counterparties, which are of systemically relevant size.   Keywords:  Correlation stress testing, scenario selection, market risk, "London Whale"  JEL Classication:  , , , 
SEP
Abstract In a continuous-time setting where a risk-averse agent controls the drift of an output process driven by a Brownian motion, optimal contracts are linear in the terminal output; this result is well-known in a setting with moral hazard and  under stronger assumptions  adverse selection. We show that this result continues to hold when in addition reser- vation utilities are type-dependent. This type of problem occurs in the study of optimal compensation problems involving competing principals.   Keywords:  Principal-agent modelling; contract design; stochastic process; stochastic control
SEP
Abstract In this paper, we study the latent group structure in cryptocurrencies market by forming a dynamic return inferred network with coin attributions. We develop a dynamic covariate-assisted spectral clustering method to detect the communities in dynamic network framework and prove its uniform consistency along the horizons. Applying our new method, we show the return inferred network structure and coin attributions, including algorithm and proof types, jointly determine the market segmentation. Based on the network model, we propose a novel \hard-to-value" measure using the centrality scores. Further analysis reveals that the group with a lower centrality score exhibits stronger short-term return reversals. Cross-sectional return predictability further conrms the economic meanings of our grouping results and reveal important portfolio management implications.   Keywords:  Community Detection, Dynamic Network, Return Predictability, Behavioural Bias, Market Segmentation, Bitcoin
SEP
Abstract IV regression in the context of a re-sampling is considered in the work. Comparatively, the contribution in the development is a structural identication in the IV model. The work also contains a multiplier-bootstrap justication.   Keywords:  Gaussian Process, Kernel methods, Wasserstein Distance 
SEP
Abstract In this work, we propose to define Gaussian Processes indexed by multidimensional distributions. In the framework where the distributions can be modeled as i.i.d realizations of a measure on the set of distributions, we prove that the kernel defined as the quadratic distance between the transportation maps, that transport each distribution to the barycenter of the distributions, provides a valid covariance function. In this framework, we study the asymptotic properties of this process, proving micro ergodicity of the parameters.   Keywords:  Gaussian Process, Kernel methods, Wasserstein Distance  
SEP
Abstract We consider a problem of multiclass classification, where the training sample Sn = {(Xi, Yi)}n i=1 is generated from the model P(Y = m|X = x) = m(x), 1 6 m 6 M, and 1(x), . . . , M(x) are unknown Lip- schitz functions. Given a test point X, our goal is to estimate 1(X), . . . , M(X). An approach based on nonparametric smoothing uses a localization technique, i.e. the weight of observation (Xi, Yi) depends on the distance between Xi and X. However, local estimates strongly depend on localiz- ing scheme. In our solution we fix several schemes , . . . ,WK, compute corresponding local estimates e(1), . . . , e(K) for each of them and apply an aggregation procedure. We propose an algorithm, which constructs a con- vex combination of the estimates e(1), . . . , e(K) such that the aggregated estimate behaves approximately as well as the best one from the collection e(1), . . . , e(K). We also study theoretical properties of the procedure, prove oracle results and establish rates of convergence under mild assumptions.   Keywords:    
SEP
Abstract In the work a characterization of difference of multivariate Gaussian measures is found on the family of centered Eucledian balls. In particular, it helps to derive (xx see paper).   Keywords:  multivariate Gaussian measure, Kolmogorov distance, Gaussian comparison  
SEP
Abstract Let ; : : : ;Xn be i.i.d. sample in Rp with zero mean and the covariance matrix  . The classic principal component analysis esti- mates the projector P J onto the direct sum of some eigenspaces of  by its empirical counterpart bPJ . Recent papers [20, 23] investigate the asymptotic distribution of the Frobenius distance between the projectors k bPJ ??P J  . The problem arises when one tries to build a condence set for the true projector eectively. We consider the problem from Bayesian perspective and derive an approximation for the posterior distribution of the Frobenius distance between projectors. The derived theorems hold true for non-Gaussian data: the only assumption that we impose is the con- centration of the sample covariance b in a vicinity of  . The obtained results are applied to construction of sharp condence sets for the true pro- jector. Numerical simulations illustrate good performance of the proposed procedure even on non-Gaussian data in quite challenging regime.   Keywords:  covariance matrix, spectral projector, principal component analysis, Bernstein { von Mises theorem.  
SEP
Fehler Die Verwendung von Cookies ist nicht erlaubt. Sie müssen Cookies erlauben, bevor Sie sich anmelden können. Benutzername Passwort Passwort vergessen? Sollten Sie Ihr Passwort vergessen haben,  https://www.cms.hu-berlin.de/portale/studierende/PasswortVergessen_html .
SEP
Abstract In this paper, we consider a probabilistic setting where the probability measures are considered to be random objects. We propose a procedure of construction non-asymptotic confidence sets for empirical barycenters in 2 -Wasserstein space and develop the idea further to construction of a non-parametric two-sample test that is then applied to the detection of structural breaks in data with complex geometry. Both procedures mainly rely on the idea of multiplier bootstrap (Spokoiny and Zhilova [29], Chernozhukov, Chetverikov and Kato [13]). The main focus lies on probability measures that have commuting covariance matrices and belong to the same scatter-location family: we proof the validity of a bootstrap procedure that allows to compute confidence sets and critical values for a Wasserstein-based two-sample test.   Keywords:  Wasserstein barycenters, hypothesis testing, multiplier bootstrap, change point detection, confidence sets.  
SEP
Abstract Let , . . . ,Xn be i.i.d. sample in Rp with zero mean and the covariance matrix . The problem of recovering the projector onto an eigenspace of  from these observations naturally arises in many applications. Recent technique from [9] helps to study the asymp- totic distribution of the distance in the Frobenius norm kPr - bP r between the true projector Pr on the subspace of the rth eigenvalue and its empirical counterpart bP r in terms of the effective rank of . This paper offers a bootstrap procedure for building sharp confidence sets for the true projector Pr from the given data. This procedure does not rely on the asymptotic distribution of kPr - bP r and its moments. It could be applied for small or moderate sample size n and large dimension p. The main result states the validity of the proposed procedure for finite samples with an explicit error bound for the er- ror of bootstrap approximation. This bound involves some new sharp results on Gaussian comparison and Gaussian anti-concentration in high-dimensional spaces. Numeric results confirm a good performance of the method in realistic examples.   Keywords:    
SEP
Abstract We distill sentiment from a huge assortment of NASDAQ news articles by means of machine learning methods and examine its predictive power in single-stock option markets and equity markets. We provide evidence that single-stock options react to contemporaneous sentiment. Next, examining return predictability, we discover that while option variables indeed predict stock returns, sentiment variables add further informational content. In fact, both in a regression and a trading context, option variables orthogonalized to public and sentimental news are even more informative predictors of stock returns. Distinguishing further between overnight and trading-time news, we find the first to be more informative. From a statistical topic model, we uncover that this is attributable to the differing thematic coverage of the alternate archives. Finally, we show that sentiment disagreement commands a strong positive risk premium above and beyond market volatility and that lagged returns predict future returns in concentrated sentiment environments.   Keywords:  investor disagreement; option markets; overnight information; stock return predictability; textual sentiment; topic model; trading-time information;  
SEP
Abstract The New Keynesian theory of inflation determination has been under scrutiny due to identification issues, which rather have to do with the mechanism of inflation determination at its core (i.e. Cochrane (2011)). Moreover, similar identification problems arise in the case of fiscal inflation (see for example Leeper and Leith (2016), Leeper and Li (2017) and Leeper and Walker (2012)). This paper makes a positive contribution. We argue that statements about observational equivalence stem from referring to the equilibrium path, while this should not be our primary source of identifying restrictions. Moreover, policy identification (or lack thereof) relies on assumptions on the underlying shock structure, which is unobservable. We instead extract shocks using heterogeneous uncertain restrictions and external datasets, that is, we learn from errors. We are then able to recover deep and policy parameters irrespective of the prevailing equilibrium. We provide time varying evidence on the efficacy of policy in stabilizing the US economy and on the time varying plausibility of Ricardian versus non-Ricardian price determination. Results are work in progress.   Keywords:  Monetary and fiscal policy, Price Determination, Identification, Learning from errors  
SEP
Abstract We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors.   Keywords:  LASSO, time series, simultaneous inference, system of equations, Z-estimation, Bahadur representation, martingale decomposition  
SEP
Abstract In this paper, we propose a new class of regime shift models with  exible switching mechanism that relies on a nonparametric probability function of the observed thresh- old variables. The proposed models generally embrace traditional threshold models with contaminated threshold variables or heterogeneous threshold values, thus gaining more power in handling complicated data structure. We solve the identification issue by imposing either global shape restriction or boundary condition on the nonparametric probability function. We utilize the natural connection between penalized splines and hierarchical Bayes to conduct smoothing. By adopting dierent priors, our procedure could work well for estimations of smooth curve as well as discontinuous curves with occasionally structural breaks. Bayesian tests for the existence of threshold eects are also conducted based on the posterior samples from Markov chain Monte Carlo (M- CMC) methods. Both simulation studies and an empirical application in predicting the U.S. stock market returns demonstrate the validity of our methods.   Keywords:  Threshold Model, Nonparametric, Markov Chain Monte Carlo, Bayesian Inference, Spline.  
SEP
Abstract Given data y and k covariates xj one problem in linear regression is to decide which if any of the covariates to include when regressing the dependent variable y on the covariates xj . In this paper three such methods, lasso, knockoff and Gaussian covariates are compared using simulations and real data. The Gaussian covariate method is based on exact probabilities which are valid for all y and xj making it model free. Moreover the probabilities agree with those based on the F-distribution for the standard linear model with i.i.d. Gaussian errors. It is conceptually, mathematically and algorithmically very simple, it is very fast and makes no use of simulations. It outperforms lasso and knockoff in all respects by a considerable margin.   Keywords:    
SEP
Abstract This paper presents a new approach to non-parametric cluster analysis called Adaptive Weights Clustering (AWC). The idea is to identify the clustering structure by checking at different points and for dierent scales on departure from local homogeneity. The proposed procedure describes the clustering structure in terms of weights wij each of them measures the degree of local inhomogeneity for two neighbor local clusters using statistical tests of \no gap" between them. The procedure starts from very local scale, then the parameter of locality grows by some factor at each step. The method is fully adaptive and does not require to specify the number of clusters or their structure. The clustering results are not sensitive to noise and outliers, the procedure is able to recover dierent clusters with sharp edges or manifold structure. The method is scalable and computationally feasible. An intensive numerical study shows a state-of-the-art performance of the method in various articial examples and applications to text data. Our theoretical study states optimal sensitivity of AWC to local inhomogeneity.   Keywords:  adaptive weights, clustering, gap coecient, manifold clustering  
SEP
Abstract We investigate the concept of connectedness, which is important for risk measurement and management inGerman energy market. Understanding and learning from these mechanisms are essential to avoid future systemic disasters. To deal with large portfolio selection, we propose regularization approach to capture the spillover and contagion effects acrossGerman power derivatives. This paper shows how network analysis can facilitate the monitoring of futures price movements. Our methodology combines high-dimensional variable selection techniques with network analysis, the results show that contracts like Phelix Base Year Options and Phelix Peak Year Futures are in the core of the Energy futures market.   Keywords:  regularization, energy risk transmission, network, German energy market  
SEP
Abstract This paper analyzes the market impact of limit order books (LOB) taking crossstock effects into account. Based on penalized vector autoregressive approach, we aim to identify significance and magnitude of the directed network channels within and between LOBs by bootstrapped impulse response functions. Moreover, information on asymmetries and imbalances within the LOB over time would be derived. For the sample of a NASDAQ blue-chip portfolio during 06-07/2016 we find that LOB network effects crucially determine prices and bid-ask asymmetries are prevalent.   Keywords:  limit order book, high dimension, generalized impulse response, high frequency, market risk, market impact, network, bootstrap  
SEP
Abstract Cryptocurrencies such as Bitcoin are establishing themselves as an investment asset and are often named the New Gold. This study, however, shows that the two assets could barely be more dierent. Firstly, we analyze and compare conditional variance properties of Bitcoin and Gold as well as other assets and nd dierences in their structure. Secondly, we implement a BEKK-GARCH model to estimate time-varying conditional correlations. Gold plays an important role in nancial markets with  ight-to-quality in times of market distress. Our results show that Bitcoin behaves as the exact opposite and it positively correlates with downward markets. Lastly, we analyze the properties of Bitcoin as portfolio component and nd no evidence for hedging capabilities. We conclude that Bitcoin and Gold feature fundamentally dierent properties as assets and linkages to equity markets. Our results hold for the broad cryptocurrency index CRIX. As of now, Bitcoin does not re ect any distinctive properties of Gold other than asymmetric response in variance.   Keywords:  BEKK, Bitcoin, CRIX, Cryptocurrency, Gold, GARCH, Conditional Correlation, Asymmetry, Long memory  
SEP
Abstract Trading of Bitcoin is spread about multiple venues where buying and selling is offered in various currencies. However, all markets trade one common good and by the law of one price, the different prices should not deviate in the long run. In this context we are interested in which platform is the most important one in terms of price discovery. To this end, we use a pairwise approach accounting for a potential impact of exchange rates. The contribution to price discovery is measured by Hasbrouck's and Gonzalo and Granger's information share. We then derive an ordering with respect to the importance of each market which reveals that the Chinese OKCoin platform is the leader in price discovery of Bitcoin, followed by BTC China.   Keywords:  price discovery; Bitcoin; Hasbrouck information shares;  
SEP
Abstract Data from social media has created opportunities to understand how and why people move through their urban environment and how this relates to criminal activity. To aid resource allocation decisions in the scope of predictive policing, the paper proposes an approach to predict weekly crime counts. The novel approach captures spatial dependency of criminal activity through approximating human dynamics. It integrates point of interest data in the form of Foursquare venues with Twitter activity and taxi trip data, and introduces a set of approaches to create features from these data sources. Empirical results demonstrate the explanatory and predictive power of the novel features. Analysis of a six-month period of real-world crime data for the city of New York evidences that both temporal and static features are necessary to eectively account for human dynamics and predict crime counts accurately. Furthermore, results provide new evidence into the underlying mechanisms of crime and give implications for crime analysis and intervention.   Keywords:  Predictive Policing, Crime Forecasting, Social Media Data, Spatial Econometrics  
SEP
Abstract Marketing messages are most effective if they reach the right customers. Deciding which customers to contact is thus an important task in campaign planning. The paper focuses on empirical targeting models. We argue that common practices to develop such models do not account sufficiently for business goals. To remedy this, we propose profit-conscious ensemble selection, a modeling framework that integrates statistical learning principles and business objectives in the form of campaign profit maximization. The results of a comprehensive empirical study confirm the business value of the proposed approach in that it recommends substantially more profitable target groups than several benchmarks.   Keywords:  Marketing Decision Support, Business Value, Profit-Analytics, Machine Learning  
SEP
Abstract New Public Management helps universities and research institutions to perform in a highly competitive research environment. Evaluating publicly financed research results improves transparency, helps in reflection and self-assessment, and provides information for strategic decision making. In this paper we provide empirical evidence using data from a Collaborative Research Centre (CRC) on financial inputs and research output from 2005 to 2016. After selecting performance indicators suitable for a CRC, we describe main properties of the data using visualization techniques. To study the relationship between the dimensions of research performance, we use a time fixed effects panel data model and fixed effects Poisson model. With the help of year dummy variables, we show how the pattern of research productivity changed over time after controlling for staff and travel costs. The joint depiction of the time fixed effects and the research projects life cycle allows a better understanding of the development of the number of discussion papers over time.   Keywords:  Research Performance, Time Fixed Effects Panel Data Model, Fixed Effects Poisson Model, Network, Collaborative Research Centre  
SEP
Abstract Estimation or mis-specification errors in the portfolio loss distribution can have a considerable impact on risk measures. This paper investigates the sensitivity of tail-related risk measures including the Value-at-Risk, expected shortfall and the expectile-quantile transformation level in an epsiloncontamination neighbourhood. The findings give the different approximations via the tail heaviness of the contamination models and its contamination levels. Illustrating examples and an empirical study on the dynamic CRIX capturing and displaying the market movements are given. The codes used to obtain the results in this paper are available via https://github.com/QuantLet/SRMC   Keywords:  Sensitivity, expected shortfall, expectile, Value-at-Risk, risk management, influence function, CRIX  
SEP
Abstract Many Southeast European countries are currently undergoing a process of liberalization of electric power markets. The paper analyses day-ahead price dynamics on some of these new markets and in Germany as a benchmark of a completely decentralized Western European market. To that end, several price forecasting methods including autoregressive approaches, multiple linear regression, and neural networks are considered. These methods are tested on hourly day-ahead price data during four two-week periods corresponding to different seasons and varying levels of volatility in all selected markets. The most influential fundamental factors are determined and performance of forecasting techniques is analysed with respect to the age of the market, its degree of liberalization, and the level of volatility. A comparison of Southeast European electricity markets of different age with the older German market is made and clusters of similar Southeast European markets are identified.   Keywords:  ARIMA models, energy forecasting, time series models, neural networks  
SEP
Abstract The recent emergence of blockchain-based cryptocurrencies has received a considerable attention. The growing acceptance of cryptocurrencies has led many to speculate that the blockchain technology can surpass a traditional centralized monetary system. However, no monetary model has yet been de- veloped to study the economics of the blockchain. This paper builds a model of the economy with a single generally acepted blockchain-based currency. In the spirit of the search and matching literature I use a matching function to model the operation of the blockchain. The formulation of the money demand is taken from a workhorse of monetary economics - Lagos and Wright (2005). I show that in a blockchain-based monetary system money demand features a precautionary motive which is absent in the standard Lagos-Wright model. Due to this precautionary money demand the monetary equilibrium can be stable for some calibrations. I also used the developed model to study how the equilibrium return on money is  Keywords:  Blockchain, Miners, Cryptocurrency, Matching function  
SEP
Abstract We link the hiring of R&D scientists from industry competitors to the subsequent formation of collaborative agreements, namely technology-oriented alliances. By transferring technological knowledge as well as cognitive elements to the hiring firm, mobile inventors foster the alignment of decision frames applied by potential alliance partners in the process of alliance formation thereby making collaboration more likely. Using data on inventor mobility and alliance formation amongst 42 global pharmaceutical firms over 16 years, we show that inventor mobility is positively associated with the likelihood of alliance formation in periods following inventor movements. This relationship becomes more pronounced if mobile employees bring additional knowledge about their prior firms technological capabilities and for alliances aimed at technology development rather than for agreements related to technology transfer. It is weakened, however, if the focal firm is already familiar with the competitors technological capabilities. By revealing these relationships, our study contributes to research on alliance formation, employee mobility, and organizational frames.   Keywords:    JEL classication:
SEP
Abstract The recent development of private cryptocurrencies has created a need to extend existing models of private currency provision and currency competi- tion. The outcome of cryptocurrency competition should be analyzed in a model which incorporates important features of the modern cryptocurren- cies. In this paper I focus on two such features. First, cryptocurrencies operate according to a protocol - a blockchain - and are, therefore, free from the time-inconsistency problem. Second, the operation of the blockchain costs real resources. I use the Lagos-Wright search theoretic monetary model augmented with privately issued currencies as in Fernandez-Villaverde and Sanches (2016) and extend it by linear costs of private currency circulation. I show that in contrast to Fernandez-Villaverde and Sanches (2016) cryptocur- rency competition 1) does not deliver price stability and 2) puts downward pressure on the in ation in the public currency only when the costs private currency circulation (mining costs) are suciently low.   Keywords:  Currency competition, Cryptocurrency, In ation, Blockchain  JEL classication:  , , , 
SEP
Testing for bubbles in cryptocurrencies with time-varying volatility Christian M. Hafner Abstract The recent evolution of cryptocurrencies has been characterized by bubble-like behavior and extreme volatility. While it is difficult to assess an intrinsic value to a specific cryptocurrency, one can employ recently proposed bubble tests that rely on recursive applications of classical unit root tests. This paper extends this approach to the case where volatility is time varying, assuming a deterministic longrun component that may take into account a decrease of unconditional volatility when the cryptocurrency matures with a higher market dissemination. Volatility also includes a stochastic short-run component to capture volatility clustering. The wild bootstrap is shown to correctly adjust the size properties of the bubble test, which retains good power properties. In an empirical application using eleven of the largest cryptocurrencies and the CRIX index, the general evidence in favor of bubbles is confirmed, but much less pronounced than under constant volatility. Keywords:  cryptocurrencies, speculative bubbles, wild bootstrap, volatility , , 
SEP
Abstract The CRIX (CRyptocurrency IndeX) has been constructed based on a number of cryptos and provides a high coverage of market liquidity, hu.berlin/crix. The crypto currency market is a new asset market and attracts a lot of investors recently. Surprisingly a market for contingent claims hat not been built up yet. A reason is certainly the lack of pricing tools that are based on solid financial econometric tools. Here a first step towards pricing of derivatives of this new asset class is presented. After a careful econometric pre-analysis we motivate an affine jump diffusion model, i.e., the SVCJ (Stochastic Volatility with Correlated Jumps) model. We calibrate SVCJ by MCMC and obtain interpretable jump processes and then via simulation price options. The jumps present in the cryptocurrency fluctutations are an essential component. Concrete examples are given to establish an OCRIX exchange platform trading options on CRIX.   Keywords:  CRyptocurrency IndeX, CRIX, Bitcoin,Cryptocurrency, SVCJ, Option pricing,OCRIX
SEP
Abstract With option-implied volatility indices, we provide a new tool for event studies in a network setting and document systemic risk in the spillover networks across global financial markets. Network linkages are sufficiently asymmetric because the US stock and bond markets play as dominant volatility suppliers to other countries and markets. Shocks from the US generate systemic risk through intensifying volatility spillovers across countries and asset classes. The findings offer new evidence that asymmetric network linkages can lead to sizable aggregate fluctuations and thus potential systemic risk.   Keywords:  Network; Option-implied Volatility; Spillover; Asymmetric linkage; Systemic risk
SEP
Abstract: For multivariate nonparametric regression models, existing variable selection methods with penalization require high-dimensional nonparametric approximations in objective functions. When the dimension is high, none of methods with penalization in the literature are readily available. Also, ranking and screening approaches cannot have selection consistency when iterative algorithms cannot be used due to inefficient nonparametric approximation. In this paper, a novel and easily implemented approach is proposed to make existing methods feasible for selection with no need of nonparametric approximation. Selection consistency can be achieved. As an application to additive regression models, we then suggest a two-stage procedure that separates selection and estimation steps. An adaptive estimation to the smoothness of underlying components can be constructed such that the consistency can be even at parametric rate if the underlying model is really parametric. Simulations are carried out to examine the performance of our method, and a real data example is analyzed for illustration.   Keywords: Adaptive estimation; non-parametric additive model; purely nonparametric regression; variable selection
SEP
Abstract: Appropriate risk management is crucial to ensure the competitiveness of financial institutions and the stability of the economy. One widely used financial risk measure is Value-at-Risk (VaR). VaR estimates based on linear and parametric models can lead to biased results or even underestimation of risk due to time varying volatility, skewness and leptokurtosis of nancial return series. The paper proposes a nonlinear and nonparametric framework to forecast VaR. Mean and volatility are modeled via support vector regression (SVR) where the volatility model is motivated by the standard generalized autoregressive conditional heteroscedasticity (GARCH) formulation. Based on this, VaR is derived by applying kernel density estimation (KDE). This approach allows for  exible tail shapes of the profit and loss distribution and adapts for a wide class of tail events. The SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold GARCH models coupled with different error distributions. To examine the performance in different markets, one-day-ahead forecasts are produced for different financial indices. Model evaluation using a likelihood ratio based test framework for interval forecasts indicates that the SVR-GARCH-KDE hybrid performs competitive to benchmark models. Especially models that are coupled with a normal distribution are systematically outperformed.   Keywords: Value-at-Risk, Support Vector Regression, Kernel Density Estimation, GARCH
SEP
Abstract:The management of universities requires data on teaching and research performance.While teaching quality can be measured via student performance and teacher evaluationprograms, the connection of research outputs and their antecedents is much harder tocheck, test and understand. To inform research governance and policy making at universities, the paper clarifies the relationship between grant money and researchperformance. We examine the interdependence structure between third-party expenses (TPE),publications, citations and academic age. To describe the relationship between these factors, we analyze individual level data from a sample of professorships from a leadingresearch university and a Scopus database for the period 2001 to 2015. Using estimatesfrom a PVARX model, impulse response functions and a forecast error variance decomposition, we show that an analysis at the university level is inappropriate anddoes not reflect the behavior of individual faculties. We explain the differences in the relationship structure between indicators for social sciences and humanities, life sciences and mathematical and natural sciences. For instance, for mathematics andsome fields of social sciences and humanities, the influence of TPE on the number of publications is insignificant, whereas the influence of TPE on the number of citationsis significant and positive. Corresponding results quantify the difference between the quality and quantity of research outputs, a better understanding of which is important to design incentive schemes and promotion programs. The paper also proposes a visualization of the cooperation between faculties and research interdisciplinarity viathe co-authorship structure among publications. We discuss the implications for policyand decision making and make recommendations for the research management of universities.Keywords:causal inference, sample splitting, cross-fitting, sample averaging, machine learning,simulation study
SEP
Abstract:High-frequency data can provide us with a quantity of information for forecasting, help to calculate and prevent the future risk based on extremes. This tail behaviour is very often driven by exogenous components and may be modelled conditional on other variables. However, many of these phenomena are observed over time, exhibiting non-trivial dynamics and dependencies. We propose a functional dynamic factor model to study the dynamics of expectile curves. The complexity of the model and the number of dependent variables are reduced by lasso penalization. The functional factors serve as a low-dimensional representation of the conditional tail event, while the time-variation is captured by factor loadings. We illustrate the model with an application to climatology, where daily data over years on temperature, rainfalls or strength of wind are available.Keywords:factor model, functional data, expectiles, extremes
SEP
Abstract:For change-point analysis of high dimensional time series, we consider a semiparametric model with dynamic structural break factors. The observations are described by a few low dimensional factors with time-invariate loading functions of covariates. The unknown structural break in time models the regime switching effects introduced by exogenous shocks. In particular, the factors are assumed to be nonstationary and follow a Vector Autoregression (VAR) process with a structural break. In addition, to account for the known spatial discrepancies, we introduce discrete loading functions. We study the theoretical properties of the estimates of the loading functions and the factors. Moreover, we provide both the consistency and the asymptotic convergence results for making inference on the common breakpoint in time. The estimation precision is evaluated via a simulation study. Finally we present two empirical illustrations on modeling the dynamics of the minimum wage policy in China and analyzing a limit order book dataset.Keywords:high dimensional time series, change-point analysis, temporal and cross-sectional dependence, vector autoregressive process
SEP
Abstract:The EU Emission Trading System (EU ETS) was created to reduce the C and other greenhouse gas emissions at the lowest economic cost. In reality market participants are faced with considerable uncertainty due to price changes and require price and volatility estimates and forecasts for appropriate risk management, asset allocation and volatility trading. Although the simplest approach to estimate volatility is to use the historical standard deviation, realized volatility is a more accurate measure for volatility, since it is based on intraday data. Besides the stylized facts commonly observed in financial time series, we observe long-memory properties in the realized volatility series, which motivates the use of Heterogeneous Autoregressive (HAR) class models. Therefore, we propose to model and forecast the realized volatility of the EU ETS futures with HAR class models. The HAR models outperform benchmark models such as the standard long-memory ARFIMA model in terms of model fit, in-sample and out-of-sample forecasting. The analysis is based on intraday data (May 2007-April 2012) for futures on C certificates for the second EU-ETS trading period (expiry December 2008-2012). The estimation results of the models allow to explain the volatility drivers in the market and volatility structure, according to the Heterogeneous Market Hypothesis as well as the observed asymmetries. We see that both speculators with short investment horizons as well as traders taking long-term hedging positions are active in the market. In a simulation study we test the suitability of the HAR model for option pricing and conclude that the HAR model is capable of mimicking the long-term volatility structure in the futures market and can be used for short-term and long-term option pricing.Keywords:EU ETS, Realized Volatility, HAR, Volatility Forecasting, Intraday Data, C Emission Allowances, Emissions Markets, Asymmetry, SHAR, HARQ, MC Simulation
SEP
Abstract:This paper considers a fast and effective algorithm for conducting functional principle component analysis with multivariate factors. Compared with the univariate case, our approach could be more powerful in revealing spatial connections or extracting important features in images. To facilitate fast computation, we connect Singular Value Decomposition with penalized smoothing and avoid estimating a huge dimensional covariance operator. Under regularity assumptions, the results indicate that we may enjoy the optimal convergence rate by employing the smoothness assumption inherent to functional objects. We apply our method on the analysis of brain image data. Our extracted factors provide excellent recovery of the risk related regions of interests in human brain and the estimated loadings are very informative in revealing the individual risk attitude.Keywords:Principal Component Analysis, Penalized Smoothing, Asymptotics, functional Magnetic Resonance Imaging (fMRI)
SEP
Abstract:In the present paper we propose a new method, the Penalized Adaptive Method (PAM), for a data driven detection of structure changes in sparse linear models. The method is able to allocate the longest homogeneous intervals over the data sample and simultaneously choose the most proper variables with help of penalized regression models. The method is simple yet flexible and can be safely applied in high-dimensional cases with different sources of parameter changes. Comparing with the adaptive method in linear models, its combination with dimension reduction yields a method which selects proper significant variables and detects structure breaks while steadily reduces the forecast error in high-dimensional data. When applying PAM to bond risk premia modelling, the locally selected variables and their estimated coefficient loadings identified in the longest stable subsamples over time align with the true structure changes observed throughout the market.Keywords:SCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap, bond risk premia
SEP
Abstract:In the present paper we propose a new method, the Penalized Adaptive Method (PAM), for a data driven detection of structure changes in sparse linear models. The method is able to allocate the longest homogeneous intervals over the data sample and simultaneously choose the most proper variables with help of penalized regression models. The method is simple yet flexible and can be safely applied in high-dimensional cases with different sources of parameter changes. Comparing with the adaptive method in linear models, its combination with dimension reduction yields a method which selects proper significant variables and detects structure breaks while steadily reduces the forecast error in high-dimensional data. When applying PAM to bond risk premia modelling, the locally selected variables and their estimated coefficient loadings identified in the longest stable subsamples over time align with the true structure changes observed throughout the market.Keywords:SCAD penalty, propagation-separation, adaptive window choice, multiplier bootstrap, bond risk premia
SEP
Abstract:Systemic risk quantification in the current literature is concentrated on market-based methods such as CoVaR(Adrian and Brunnermeier (2016)). Although it is easily implemented, the interactions among the variables of interest and their joint distribution are less addressed. To quantify systemic risk in a system-wide perspective, we propose a network-based factor copula approach to study systemic risk in a network of systemically important financial institutions (SIFIs). The factor copula model offers a variety of dependencies/tail dependencies conditional on the chosen factor; thus constructing conditional network. Given the network, we identify the most “connected” SIFI as the central SIFI, and demonstrate that its systemic risk exceeds that of non-central SIFIs. Our identification of central SIFIs shows a coincidence with the bucket approach proposed by the Basel Committee on Banking Supervision, but places more emphasis on modeling the interplay among SIFIs in order to generate systemwide quantifications. The network defined by the tail dependence matrix is preferable to that defined by the Pearson correlation matrix since it confirms that the identified central SIFI through it severely impacts the system. This study contributes to quantifying and ranking the systemic importance of SIFIs.Keywords:factor copula, network, Value-at-Risk, tail dependence, eigenvector centrality
SEP
Abstract:With increasing wind power penetration more and more volatile and weather dependent energy is fed into the German electricity system. To manage the risk of windless days and transfer revenue risk from wind turbine owners to investors wind power derivatives were introduced. These insurance-like securities (ILS) allow to hedge the risk of unstable wind power production on exchanges like Nasdaq and European Energy Exchange. These products have been priced before using risk neutral pricing techniques. We present a modern and powerful methodology to model weather derivatives with very skewed underlyings incorporating techniques from extreme event modelling to tune seasonal volatility and compare transformed Gaussian and non-Gaussian CARMA(p; q) models. Our results indicate that the transformed Gaussian CARMA(p; q) model is preferred over the non-Gaussian alternative with Lévy increments. Out-of-sample backtesting results show good performance wrt burn analysis employing smooth Market Price of Risk (MPR) estimates based on NASDAQ weekly and monthly German wind power futures prices and German wind power utilisation as underlying. A seasonal MPR of a smile-shape is observed, with positive values in times of high volatility, e.g. winter months, and negative values, in times of low volatility and production, e.g. in summer months. We conclude that producers pay premiums to insure stable revenue steams, while investors pay premiums when weather risk is high.Keywords:market price of risk, risk premium, renewable energy, wind power futures, stochastic process, expectile, CARMA, jump, Lévy, transform, logit-normal, extreme
SEP
Abstract:Evidence from the American Time Use Survey 2003-12 suggests the existence of small butstatistically significant racial/ethnic differences in time spent not working at the workplace. Minorities, especially men, spend a greater fraction of their workdays not working than do white non-Hispanics. These differences are robust to the inclusion of large numbers of demographic, industry, occupation, time and geographic controls. They do not vary by union status, public-private sector attachment, pay method or age; nor do they arise from the effects of equal-employment enforcement or geographic differences in racial/ethnic representation. The findings imply that measures of the adjusted wagedisadvantages of minority employees are overstated by about 10 percent.Keywords:time use, wage discrimination, wage differentials
SEP
Abstract:Cryptocurrencies have left the dark side of the finance universe and become an object of study for asset and portfolio management. Since they have a low liquidity compared to traditional assets, one needs to take into account liquidity issues when one puts them into the same portfolio. We propose use a Liquidity Bounded Risk-return Optimization (LIBRO) approach, which is a combination of the Markowitz framework under the liquidity constraints. The results show that cryptocurrencies add value to a portfolio and the optimization approach is even able to increase the return of a portfolio and lower the volatility risk.Keywords:crypto-currency, CRIX, portfolio investment, asset classes, blockchain
SEP
AbstractAbstract) in terms of cluster membership. Comparison with k-means or CLUTO reveals excellent performance of AWC.Keywords:Clustering, JEL system, Adaptive algorithm, Economic articles, Nonparametric
SEP
Abstract:This paper contributes to model the industry interconnecting structure in a network context. General predictive model (Rapach et al. 2016) is extended to quantile LASSO regression so as to incorporate tail risks in the construction of industry interdependency networks. Empirical results show a denser network with heterogeneous central industries in tail cases. Network dynamics demonstrate the variety of interdependency across time. Lower tail interdependency structure gives the most accurate out-of-sample forecast of portfolio returns and network centrality-based trading strategies seem to outperform market portfolios, leading to the possible ’too central to fail’ argument.Keywords:dynamic network, interdependency, general predictive model, quantile LASSO, connectedness, centrality, prediction accuracy, network-based trading strategy
SEP
Abstract:Data Science looks at raw numbers and informational objects created by different disciplines. The Digital Society creates information and numbers from many scientific disciplines. The amassment of data though makes is hard to Hind structures and requires a skill full analysis of this massive raw material. The thoughts presented here on D - Data Science & Digital Society analyze these challenges and offers ways to handle the questions arising in this evolving context. We propose three levels of analysis and lay out how one can react to the challenges that come about. Concrete examples concern Credit default swaps, Dynamic Topic modeling, Crypto currencies and above all the quantitative analysis of real data in a D context.Keywords:Data Science, Digital Society, social networks, herding, sentiments
SEP
Abstract:This paper reviews the performance of the East German economy in the turbulent quarter-century following reunification and draws some conclusions for the reunification of North and South Korea. In this period, the gap in output per capita between East and West Germany declined at a speed not far from empirical estimates of the neoclassical growth model, yet systematic total factor productivity differentials persist despite identical institutional frameworks andsignificant investment in the eastern regions. At the same time, regionaldisparities in income, well-being, and health are little different from those found within West Germany, and net migration has ceased. On this human metric, German unification has been an unqualified success. For Korea, an effort of this dimension will be costly. A back-of-the- envelope calculation suggests that Korean unification will cost roughly twice as much as its German counterpart.Keywords:
SEP
Abstract:This paper proposes a test for missing at random (MAR). The MAR assumption is shown to be testable given instrumental variables which are independent of response given potential outcomes. A nonparametric testing procedure based on integrated squared distance is proposed. The statistic’s asymptotic distribution under the MAR hypothesis is derived. In particular, our results can be applied to testing missing completely at random (MCAR). A Monte Carlo study examines finite sample performance of our test statistic. An empirical illustration analyzes the nonresponse mechanism in labor income questions.Keywords:Incomplete data, missing-data mechanism, selection model, nonparametric hypothesis testing, consistent testing, instrumental variable, series estimation
SEP
Abstract:The interdependence, dynamics and riskiness of financial institutions are the key features frequently tackled in financial econometrics. We propose a Tail Event driven Network Quantile Regression (TENQR) model which addresses these three aspects. More precisely, our framework captures the risk propagation and dynamics in terms of a quantile (or expectile) autoregression involving network effects quantified through an adjacency matrix. To reflect the nature and risk content of systemic risk, the construction of the adjacency matrix is suggested to include tail event covariates. The model is evaluated using the SIFIs (systemically important financial institutions) identified by the Financial Stability Board (FSB) as main players in the global financial system. The risk decomposition analysis of it identifies the systemic importance of SIFIs and thus provides measures for the required level of additional loss absorbency. It is discovered that the network effect, as a function of the tail probability, becomes more profound in stress situations and brings the various impacts to the SIFIs located in different geographic regions.Keywords:systemic risk, network analysis, network autoregression, tail event
SEP
Abstract:AI artificial intelligence brings about new quantitative techniques to assess the state of an economy. Here we describe a new measure for systemic risk: the Financial Risk Meter (FRM). This measure is based on the penalization parameter (lambda) of a linear quantile lasso regression. The FRM is calculated by taking the average of the penalization parameters over the 100 largest US publicly traded financial institutions. We demonstrate the suitability of this AI based risk measure by comparing the proposed FRM to other measures for systemic risk, such as VIX, SRISK and Google Trends. We find that mutual Granger causality exists between the FRM and these measures, which indicates the validity of the FRM as a systemic risk measure. The implementation of this project is carried out using parallel computing, the codes are published on www.quantlet.de with keyword FRM. The R package RiskAnalytics is another tool with the purpose of integrating and facilitating the research, calculation and analysis methods around the FRM project. The visualization and the up-to-date FRM can be found on hu.berlin/frm.Keywords:Systemic Risk, Quantile Regression, Value at Risk, Lasso, Parallel Computing, Financial Risk Meter
SEP
Abstract:Systemically important banks are connected and have dynamic dependencies of their default probabilities. An extraction of default factors from cross-sectional credit default swaps (CDS) curves allows to analyze the shape and the dynamics of the default probabilities. Extending the Dynamic Nelson Siegel (DNS) model, we propose a network DNS model to analyze the interconnectedness of default factors in a dynamic fashion, and forecast the CDS curves. The extracted level factors representing long-term default risk demonstrate 85.5% total connectedness, while the slope and the curvature factors document 79.72% and 62.94% total connectedness for the short-term and middle-term default risk, respectively. The issues of default spillover and systemic risk should be weighted for the market participants with longer credit exposures, and for regulators with a mission to stabilize financial markets. The US banks contribute more to the long-run default spillover before 2012, whereas the European banks are major default transmitters during and after the European debt crisis either in the long-run or short-run. The outperformance of the network DNS model indicates that the prediction on CDS curve requires network information.Keywords:CDS, network, default risk, variance decomposition, risk management
SEP
Abstract:More and more data are observed in form of curves. Numerous applications in finance, neuroeconomics, demographics and also weather and climate analysis make it necessary to extract common patterns and prompt joint modelling of individual curve variation. Focus of such joint variation analysis has been on fluctuations around a mean curve, a statistical task that can be solved via functional PCA. In a variety of questions concerning the above applications one is more interested in the tail asking therefore for tail event curves (TEC) studies. With increasing dimension of curves and complexity of the covariates though one faces numerical problems and has to look into sparsity related issues. Here the idea of Factorisable Sparse Tail Event Curves (FASTEC) via multivariate asymmetric least squares regression (expectile regression) in a high-dimensional framework is proposed. Expectile regression captures the tail moments globally and the smooth loss function improves the convergence rate in the iterative estimation algorithm compared with quantile regression. The necessary penalization is done via the nuclear norm. Finite sample oracle properties of the estimator associated with asymmetric squared error loss and nuclear norm regularizer are studied formally in this paper. As an empirical illustration, the FASTEC technique is applied on fMRI data to see if individual’s risk perception can be recovered by brain activities. Results show that factor loadings over different tail levels can be employed to predict individual’s risk attitudes.Keywords:high-dimensionalM-estimator, nuclear norm regularizer, factorization, expectile regression, fMRI, risk perception, multivariate functional data
SEP
Abstract:Cryptocurrencies are more and more used in official cash flows and exchange of goods. Bitcoin and the underlying blockchain technology have been looked at by big companies that are adopting and investing in this technology. The CRIX Index of cryptocurrencies hu.berlin/CRIX indicates a wider acceptance of cryptos. One reason for its prosperity certainly being a security aspect, since the underlying network of cryptos is decentralized. It is also unregulated and highly volatile, making the risk assessment at any given moment dicult. In message boards one finds a huge source of information in the form of unstructured text written by e.g. Bitcoin developers and investors. We collect from a popular crypto currency message board texts, user information and associated time stamps. We then provide an indicator for fraudulent schemes. This indicator is constructed using dynamic topic modelling, text mining and unsupervised machine learning. We study how opinions and the evolution of topics are connected with big events in the cryptocurrency universe. Furthermore, the predictive power of these techniques are investigated, comparing the results to known events in the cryptocurrency space. We also test hypothesis of self-fulling prophecies and herding behaviour using the results.Keywords:Dynamic Topic Modelling, Cryptocurrencies, Financial Risk 
SEP
Abstract:It is a challenging task to understand the complex dependency structures in an ultra-high dimensional network, especially when one concentrates on the tail dependency. To tackle this problem, we consider a network quantile autoregression model (NQAR) to characterize the dynamic quantile behavior in a complex system. In particular, we relate responses to its connected nodes and node specific characteristics in a quantile autoregression process. A minimum contrast estimation approach for the NQAR model is introduced, and the asymptotic properties are studied. Finally, we demonstrate the usage of our model by investigating the financial contagions in the Chinese stock market accounting for shared ownership of companies.Keywords:Social Network, Quantile Regression, Autoregression, Systemic Risk, Financial Contagion, Shared Ownership 
SEP
Abstract:In the present paper we study the dynamics of penalization parameter lambda of the least absolute shrinkage and selection operator (Lasso) method proposed by Tibshirani (1996) and extended into quantile regression context by Li and Zhu (2008). The dynamic behaviour of the parameter  can be observed when the model is assumed to vary over time and therefore the fitting is performed with the use of moving windows. The proposal of investigating time series of  and its dependency on model characteristics was brought into focus by Hardle et al. (2016), which was a foundation of FinancialRiskMeter (http://frm.wiwi.hu-berlin.de). Following the ideas behind the two aforementioned projects, we use the derivation of the formula for the penalization parameter lambda as a result of the optimization problem. This reveals three possible effects driving lambda;variance of the error term, correlation structure of the covariates and number of nonzero coefficients of the model. Our aim is to disentangle these three effect and investigate their relationship with the tuning parameter lambda, which is conducted by a simulationstudy. After dealing with the theoretical impact of the three model characteristics on lambda,empirical application is performed and the idea of implementing the parameter  into a systemic risk measure is presented. The codes used to obtain the results included in this work are available on http://quantlet.de//ia/.Keywords:Lasso, quantile regression, systemic risk, high dimensions, penalization parameter 
SEP
Abstract:Crypto-currencies have developed a vibrant market since bitcoin, the first crypto-currency, was created in 2009. We look at the properties of cryptocurrencies as financial assets in a broad cross-section. We discuss approaches of altcoins to generate value and their trading and information platforms. Then we investigate crypto-currencies as alternative investment assets, studying their returns and the co-movements of altcoin prices with bitcoin and against each other. We evaluate their addition to investors' portfolios and document they are indeed able to enhance the diversification of portfolios due to their little co-movements with established assets, as well as with each other. Furthermore, we evaluate pure portfolios of crypto-currencies: an equally weighted one, a value-weighted one, and one based on the CRypto-currency IndeX (CRIX). The CRIX portfolio displays lower risk than any individual of the liquid crypto-currencies. We also document the changing characteristics of the crypto-currency market. Deepening liquidity is accompanied by a rise in market value, and a growing number of altcoins is contributing larger amounts to aggregate crypto-currency market capitalization.Keywords:Crypto-currenciesA, ltcoins, Investment assets, Bitcoin, Blockchain, Alternative investments, Financial risk and return 
SEP
Abstract:The increasing exposure to renewable energy has amplied the need for risk management in electricity markets. Electricity price risk poses a major challenge to market participants. We propose an approach to model and forecast electricity prices taking into account information on renewable energy production. While most literature focuses on point forecasting, our methodology forecasts the whole distribution of electricity prices and incorporates spike risk, which is of great value for risk management. It is based on functional principal component analysis and time-adaptive nonparametric density estimation techniques. The methodology is applied to electricity market data from Germany. We find that renewable infeed effects both, the location and the shape of spot price densities. A comparison with benchmark methods and an application to risk management are provided.Keywords:electricity prices; residual load, probabilistic forecasting, value at risk, expected shortfall, functional data analysis 
SEP
Abstract:There are many environments in econometrics which require nonseparable modeling of a structural disturbance. In a nonseparable model, key conditions are validity of instrumental variables and monotonicity of the model in a scalar unobservable. Under these conditions the nonseparable model is equivalent to an instrumental quantile regression model. A failure of the key conditions, however, makes instrumental quantile regression potentially inconsistent. This paper develops a methodology for testing the hypothesis whether the instrumental quantile regression model is correctly specified. Our test statistic is asymptotically normally distributed under correct specification and consistent against any alternative model. In addition, test statistics to justify model simplification are established. Finite sample properties are examined in a Monte Carlo study and an empirical illustration.Keywords:Nonparametric quantile regression, instrumental variable, specification test, local alternative, nonlinear inverse problem
SEP
Abstract:The CRIX (CRyptocurrency IndeX) has been constructed based on approximately 30 cryptos and captures high coverage of available market capitalisation. The CRIX index family covers a range of cryptos based on different liquidity rules and various model selection criteria. Details of ECRIX (Exact CRIX), EFCRIX (Exact Full CRIX) and also intraday CRIX movements may be found on the webpage of hu.berlin/crix. In order to price contingent claims one needs to first understand the dynamics of these indices. Here we provide a first econometric analysis of the CRIX family within a time-series framework. The key steps of our analysis include model selection, estimation and testing. Linear dependence is removed by an ARIMA model, the diagnostic checking resulted in an ARIMA(2,0,2) model for the available sample period from Aug 1st, 2014 to April 6th, 2016. The model residuals showed the well known phenomenon of volatility clustering. Therefore a further refinement lead us to an ARIMA(2,0,2)-t-GARCH(1,1) process. This specification conveniently takes care of fat-tail properties that are typical for financial markets. The multivariate GARCH models are implemented on the CRIX index family to explore the interaction.Keywords:
SEP
Abstract:Limit order book contains comprehensive information of liquidity on bid and ask sides. We propose a Vector Functional AutoRegressive (VFAR) model to describe the dynamics of the limit order book and demand curves and utilize the fitted model to predict the joint evolution of the liquidity demand and supply curves. In the VFAR framework, we derive a closed-form maximum likelihood estimator under sieves and provide the asymptotic consistency of the estimator. In application to limit order book records of 12 stocks in NASDAQtraded from 2 Jan 2015 to 6 Mar 2015, it shows the VAR model presents a strong predictability in liquidity curves, with  values as high as 98.5 percent for insample estimation and 98.2 percent in out-of-sample forecast experiments. It produces accurate 5-; 25- and 50-minute forecasts, with root mean squared error as low as 0.09 to 0.58 and mean absolute percentage error as low as 0.3 to 4.5 percent.Keywords:Limit order book, Liquidity risk, multiple functional time series
SEP
Abstract:Mortality is different across countries, states and regions. Several empiricalresearch works however reveal that mortality trends exhibit a common pattern and show similar structures across populations. The key element in analyzing mortality rate is a time-varying indicator curve. Our main interest lies in validating the existence of the common trends among these curves, the similar gender differences and their variability in location among the curves at the national level. Motivated by the empirical findings, we make the study of estimating and forecasting mortality rates based on a semi-parametric approach, which is applied to multiple curves with the shape-related nonlinear variation. This approach allows us to capture the common features contained in the curve functions and meanwhile provides the possibility to characterize the nonlinear variation via a few deviation parameters. These parameters carry an instructive summary of the time-varying curve functions and can be further used to make a suggestive forecast analysis for countries with barren data sets. In this research the model is illustrated with mortality rates of Japan and China, and extended to incorporate more countries. All numerical procedures are transparent and reproduced on www.quantlet.de.Keywords:Nonparametric smoothing, Parametric modeling, Common trend, Mortality, Lee-Carter method, Multi-populations
SEP
Abstract:The S&0 or DA are important benchmarks for the financial industry. These and other indices describe different compositions of certain segments of the financial markets. For currency markets, the IMF offers the index SDR. Prior to the Euro, the ECU existed, which was an index representing the development of European currencies. It is surprising, though, to see that the common index providers have not mapped emerging e-coins into an index yet because with cryptos like Bitcoin, a new kind of asset of great public interest has arisen. Index providers decide on a fixed number of index constituents which will represent the market segment. It is a huge challenge to set this fixed number and develop the rules to find the constituents, especially since markets change and this has to be taken into account. A method relying on the AIC is proposed to quickly react to market changes and therefore enable us to create an index, referred to as CRIX, for the cryptocurrency market. The codes used to obtain the results in this paper are available via www.quantlet.deKeywords:Index construction, model selection, AIC, bitcoin, cryptocurrency, CRIX
SEP
Abstract:We address the problem that often hampers decision making in academic institutions – incomplete research profiles. We suggest a framework for collating ranking data of scientists for comparison purposes. As the result of an analysis of the interconnectedness between HB sub-rankings through quantile regression, we propose a HB common score for scholars within the HB community. The cross-ranking dependence analysis of Handelsblatt, Research Papers in Economics and Google Scholar ranking schemes shows that researcher age and field of specialization – mapped onto the JEL classification codes – have a substantial impact on the resulting scores.Keywords:ranking, prediction, quantile regression, Handelsblatt, RePEc, Google Scholar
SEP
Abstract:Oberwolfach Report: New Developments in Functional and Highly Multivariate Statistical MethodologyKeywords:multivariate functional data, high-dimensional M-estimators, nuclear norm regularizer, factor analysis, expectile regression, fMRI, risk perception
SEP
Abstract:This paper reviews the dramatic and widely noted developments in the German labor market in the past decade and surveys the most plausible reasons for these changes. Alternative hypotheses are compared and contrasted. I argue that the labor market reforms associated with the Agenda 2010 – the Hartz reforms – played a role at least as great as that of increasing flexibility of wage determination and the allocation of hours across workers. Until 2010, the German economic miracle could be accounted for by an expansion of part-time work, which has since been supplanted by a sustained expansion of full-time employment. Supported by wage flexibility in this segment, part-time employment represents an important new margin of flexibility in the German labor market.Keywords:German labor market miracle, Hartz reforms, part-time work, wage inequality
SEP
Abstract:In this paper, we study the statistical properties of the moneyness scaling transformation by Leung and Sircar (2015). This transformation adjusts the moneyness coordinate of the implied volatility smile in an attempt to remove the discrepancy between the IV smiles for levered and unlevered ETF options. We construct bootstrap uniform confidence bands which indicate that in a statistical sense there remains a possibility that the implied volatility smiles are still not the same, even after moneyness scaling has been performed. This presents possible arbitrage opportunities on the (L)ETF market which can be exploited by traders. We build possible arbitrage strategies by constructing portfolios with LETF shares and options which possibly have a positive value at the point of creation and non-negative value at the expiration time. An empirical data application shows that there are indeed such opportunities in the market which result in risk-free gains for the investor. A dynamic "trade-with-the-smile" strategy based on a dynamic semiparametric factor model is presented. This strategy utilizes the dynamic structure of implied volatility surface allowing out-of-sample forecasting and information on unleveraged ETF options to construct theoretical one-step-ahead implied volatility surfaces. The codes used to obtain the results in this paper, are available on www.quantlet.de.Keywords:exchange-traded funds, options, moneyness scaling, arbitrage, bootstrap, dynamic factor models
SEP
Abstract:In this paper, we suggest and analyze a new class of specification tests for random coefficient models. These tests allow to assess the validity of central structural features of the model, in particular linearity in coefficients and generalizations of this notion like a known nonlinear functional relationship. They also allow to test for degeneracy of the distribution of a random coefficient, i.e., whether a coefficient is fixed or random, including whether an associated variable can be omitted altogether. Our tests are nonparametric in nature, and use sieve estimators of the characteristic function. We analyze their power against both global and local alternatives in large samples and through a Monte Carlo simulation study. Finally, we apply our framework to analyze the specification in a heterogeneous random coefficients consumer demand model.Keywords:Nonparametric specification testing, random coefficients, unobserved heterogeneity, sieve minimum distance, characteristic function, consumer demand
SEP
Abstract:We account for time-varying parameters in the conditional expectile based value at risk (EVaR) model. EVaR appears more sensitive to the magnitude of portfolio losses compared to the quantile-based Value at Risk (QVaR), nevertheless, by fitting the models over relatively long ad-hoc fixed time intervals, research ignores the potential time-varying parameter properties. Our work focuses on this issue by exploiting the local parametric approach in quantifying tail risk dynamics. By achieving a balance between parameter variability and modelling bias, one can safely fit a parametric expectile model over a stable interval of homogeneity. Empirical evidence at three stock markets from 2005- 2014 shows that the parameter homogeneity interval lengths account for approximately 1-6 months of daily observations. Our method outperforms models with one-year fixed intervals, as well as quantile based candidates while employing a time invariant portfolio protection (TIPP) strategy for the DAX portfolio. The tail risk measure implied by our model finally provides valuable insights for asset allocation and portfolio insurance.Keywords:expectiles, tail risk, local parametric approach, risk management
SEP
Abstract:This paper addresses the problem of estimation of a nonparametric regression function from selectively observed data when selection is endogenous. Our approach relies on independence between covariates and selection conditionally on potential outcomes. Endogeneity of regressors is also allowed for. In both cases, consistent two-step estimation procedures are proposed and their rates of convergence are derived. Also pointwise asymptotic distribution of the estimators is established. In addition, we propose a nonparametric specification test to check the validity of our independence assumption. Finite sample properties are illustrated in a Monte Carlo simulation study and an empirical illustration.Keywords:Endogenous selection, instrumental variable, sieve minimum distance, regression estimation, convergence rate, asymptotic normality, hypothesis testing, inverse problem
SEP
Abstract:Inflation expectation is an important indicator for policy makers and financial investors. To capture a more accurate real-time estimate of inflation expectation on the basis of financial markets, we propose an arbitrage-free term structure model across different countries. We first estimate inflation expectation by modeling the nominal and the inflation-indexed bond yields jointly for each country. The joint dynamic model for inflation expectation is a cross sectional state space model combined with a GeoCopula model, which accounts for the default risk and the non Gaussian dependency structure over countries. We discover that the extracted common trend for inflation expectation is an important driver for each country of interest. Moreover, the model extracts informative estimates of inflation expectations and will provide good implications for monetary policies.Keywords:inflation expectation, arbitrage free, yield curve modelling, inflation risk
SEP
Abstract:The S&0 or DA are important benchmarks for the financial industry. These and other indices describe different compositions of certain segments of the financial markets. For currency markets, the IMF offers the index SDR. Prior to the Euro, the ECU existed, which was an index representing the development of European currencies. It is surprising, though, to see that the common index providers have not mapped emerging e-coins into an index yet because with cryptos like Bitcoin, a new kind of asset of great public interest has arisen. Index providers decide on a fixed number of index constituents which will represent the market segment. It is a huge challenge to set this fixed number and developthe rules to find the constituents, especially since markets change and this has to be taken into account. A method relying on the AIC is proposed to quickly react to market changes and therefore enable us to create an index, referred to as CRIX, for the cryptocurrency market.Keywords:Index construction, CRIX, risk analysis, bitcoin, cryptocurrency
SEP
Abstract:A flexible framework for the analysis of tail events is proposed. The framework contains tail moment measures that allow for Expected Shortfall (ES) estimation. Connecting the implied tail thickness of a family of distributions with the quantile and expectile estimation, a platform for risk assessment is provided. ES and implications for tail events under different distributional scenarios are investigated, particularly we discuss the implications of increased tail risk for mixture distributions. Empirical results from the US, German and UK stock markets, as well as for the selected currencies indicate that ES can be successfully estimated on a daily basis using a one-year time horizon across different risk levels.Keywords:Expected Shortfall, expectiles, tail risk, risk management, tail events, tail moments
SEP
Abstract:Classical asset allocation methods have assumed that the distribution of asset returns is smooth, well behaved with stable statistical moments over time. The distribution is assumed to have constant moments with e.g., Gaussian distribution that can be conveniently parameterised by the first two moments. However, with market volatility increasing over time and after recent crises, asset allocators have cast doubts on the usefulness of such static methods that registered large drawdown of the portfolio. Others have suggested dynamic or synthetic strategies as alternatives, which have proven to be costly to implement. The authors propose and apply a method that focuses on the left tail of the distribution and does not require the knowledge of the entire distribution, and may be less costly to implement. The recently introduced TEDAS - Tail Event Driven ASset allocation approach determines the dependence between assets at tail measures. TEDAS uses adaptive Lasso based quantile regression in order to determine an active set of portfolio elements with negative non-zero coefficients. Based on these active risk factors, an adjustment for intertemporal dependency is made. The authors extend TEDAS methodology to three gestalts differing in allocation weights’ determination: a Cornish-Fisher Value-at-Risk minimization,Markowitz diversification rule and naive equal weighting. TEDAS strategies significantly outperform other widely used allocation approaches on two asset markets: German equity and Global mutual funds.Keywords:adaptive lasso, portfolio optimisation, quantile regression, Value-at-Risk, tail events
SEP
Abstract:A standard quantitative method to access credit risk employs a factor model based on joint multi-variate normal distribution properties. By extending a one-factor Gaussian copula model to make a more accurate default forecast, this paper proposes to incorporate a state-dependent recovery rate into the conditional factor loading, and model them by sharing a unique common factor. The common factor governs the default rate and recovery rate simultaneously and creates their association implicitly. In accordance with Basel III, this paper shows that the tendency of default is more governed by systematic risk rather than idiosyncratic risk during a hectic period. Among the models considered, the one with random factor loading and a state-dependent recovery rate turns out to be the most superior on the default prediction.Keywords:Factor Model, Conditional Factor Loading, State-Dependent Recovery Rate
SEP
Abstract:We propose the use of a local autoregressive (LAR) model for adaptive estimation and forecasting of three of China’s key macroeconomic variables: GDP growth, inflation and the 7-day interbank lending rate. The approach takes into account possible structural changes in the data-generating process to select a local homogeneous interval for model estimation, and is particularly well-suited to a transition economy experiencing ongoing shifts in policy and structural adjustment. Our results indicate that the proposed method outperforms alternative models and forecast methods, especially for forecast horizons of 3 to 12 months. Our 1-quarter ahead adaptive forecasts even match the performance of the well-known CMRC Langrun survey forecast. The selected homogeneous intervals indicate gradual changes in growth of industrial production driven by constant evolution of the real economy in China, as well as abrupt changes in interest rate and inflation dynamics that capture monetary policy shifts.Keywords:Chinese economy, local parametric models, forecasting
SEP
Abstract:Estimating natural rate of unemployment (NAIRU) is important for understanding the joint dynamics of unemployment, inflation, and inflation expectation. However, existing literature falls short of endogenizing inflation expectation together with NAIRU in a model consistent way. We estimate a structural model with forward and backward looking Phillips curve. Inflation expectation is treated as a function of state variables and we use survey data as its noisy observations. Surprisingly, we find that the estimated NAIRU tracks unemployment rate closely, except for the high inflation period (late 1970s). Compared to the estimation without using the survey data, the estimated Bayesian credible sets are narrower and our model leads to better inflation and unemployment forecasts. These results suggest that monetary policy was very effective and there was not much room for policy improvement.Keywords:NAIRU, New Keynesian Phillips Curve, Inflation Expectation
SEP
Abstract:Based on the Lee-Carter (LC) model, the benchmark in population forecasting, a variety of extensions and modifications are proposed in this paper. We investigate one of the extensions, the Hyndman-Ullah (HU) method and apply it to Asian demographic data sets: China, Japan and Taiwan. It combines ideas of functional principal component analysis (fPCA), nonparametric smoothing and time series analysis. Based on this stochastic approach, the demographic characteristics and trends in different Asian regions are calculated and compared. We illustrate that China and Japan exhibited a similar demographic trend in the past decade. We also compared the HU method with the LC model. The HU method can explain more variation of the demographic dynamics when we have data of high quality, however, it also encounters problems and performs similarly as the LC model when we deal with limited and scarce data sets, such as Chinese data sets due to the substandard quality of the data and the population policy.Keywords:Functional principal component analysis; Nonparametric smoothing; Mortality forecasting; Fertility forecasting; Asian demography; Lee-Carter model, Hyndman-Ullah method
SEP
Abstract:A system of risk factors necessarily involves systemic risk. The analysis of systemic risk is in the focus of recent econometric analysis and uses tail event and network based techniques. Here we bring tail event and network dynamics together into one context. In order to pursue such joint effects, we propose a semiparametric measure to estimate systemic interconnectedness across financial institutions based on tail-driven spillover effects in a high dimensional framework. The systemically important institutions are identified conditional on their interconnectedness structure. Methodologically, a variable selection technique in a time series setting is applied in the context of a single-index model for a generalized quantile regression framework. We could thus include more financial institutions into the analysis to measure their tail event interdependencies and, at the same time, being sensitive to non-linear relationships between them. Network analysis, its behaviour and dynamics, allows us to characterize the role of each industry group in the U. S. financial market 2007 - 2012. The proposed TENET - Tail Event driven NETwork technique allows us to rank the systemic risk contributions of publicly traded U.S. financial institutions.Keywords:Systemic Risk, Systemic Risk Network, Generalized Quantile, Quantile Single-Index Regression, Value at Risk, CoVaR, Lasso
SEP
Abstract:We analyse the short-term spot price of European Union Allowances (EUAs), which is of particular importance in the transition of energy markets and for the development of new risk management strategies. Due to the characteristics of the price process, such as volatility persistence, breaks in the volatility process and heavy-tailed distributions, we investigate the use of Markov switching GARCH (MS-GARCH) models on daily spot market data from the second trading period of the EU ETS. Emphasis is given to short-term forecasting of prices and volatility. We find that MS-GARCH models distinguish well between two states and that the volatility processes in the states are clearly different. This finding can be explained by the EU ETS design. Our results support the use of MS-GARCH models for risk management, especially because their forecasting ability is better than other Markov switching or simple GARCH models.Keywords:C Emission Allowances, C Emission Trading, Spot Price Modelling, Markov Switching GARCH Models, Volatility Forecasting
SEP
Abstract:A flexible statistical approach for the analysis of time-varying dynamics of transaction data on financial markets is here applied to intra-day trading strategies. A local adaptive technique is used to successfully predict financial time series, i.e., the buyer and the seller-initiated trading volumes and the order flow dynamics. Analysing order flow series and its information content of mini Nikkei 225 index futures traded at the Osaka Securities Exchange in 2012 and 2013, a data-driven optimal length of local windows up to approximately 1-2 hours is reasonable to capture parameter variations and is suitable for short-term prediction. Our proposed trading strategies achieve statistical arbitrage opportunities and are therefore beneficial for quantitative finance practice.Keywords:multiplicative error models, trading volume, order flow, forecasting
SEP
Abstract:Portfolio selection and risk management are very actively studied topics in quantitative finance and applied statistics. They are closely related to the dependency structure of portfolio assets or risk factors. The correlation structure across assets and opposite tail movements are essential to the asset allocation problem, since they determine the level of risk in a position. Correlation alone is not informative on the distributional details of the assets. By introducing TEDAS -Tail Event Driven ASset allocation, one studies the dependence between assets at different quantiles. In a hedging exercise, TEDAS uses adaptive Lasso based quantile regression in order to determine an active set of negative non-zero coefficients. Based on these active risk factors, an adjustment for intertemporal correlation is made. Finally, the asset allocation weights are determined via a Cornish-Fisher Value-at-Risk optimization. TEDAS is studied in simulation and a practical utility-based example using hedge fund indices.Keywords:portfolio optimization, asset allocation, adaptive lasso, quantile regression, value-at-risk
SEP
Abstract:Electricity load forecasts are an integral part of many decision-making processes in the electricity market. However, most literature on electricity load forecasting concentrates on deterministic forecasts, neglecting possibly important information about uncertainty. A more complete picture of future demand can be obtained by using distributional forecasts, allowing for a more efficient decision-making. A predictive density can be fully characterized by tail measures such as quantiles and expectiles. Furthermore, interest often lies in the accurate estimation of tail events rather than in the mean or median. We propose a new methodology to obtain probabilistic forecasts of electricity load, that is based on functional data analysis of generalized quantile curves. The core of the methodology is dimension reduction based on functional principal components of tail curves with dependence structure. The approach has several advantages, such as flexible inclusion of explanatory variables including meteorological forecasts and no distributional assumptions. The methodology is applied to load data from a transmission system operator (TSO) and a balancing unit in Germany. Our forecast method is evaluated against other models including the TSO forecast model. It outperforms them in terms of mean absolute percentage error (MAPE) and achieves a MAPE of 2:7% for the TSO.Keywords:Electricity, Load forecasting, FPCA
SEP
